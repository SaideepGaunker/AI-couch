"""
Gemini AI Service for question generation and answer evaluation
"""
import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import google.generativeai as genai
from sqlalchemy.orm import Session

from app.core.config import settings
from app.db.models import Question, User
from app.schemas.question import QuestionCreate
from app.services.fallback_question_service import FallbackQuestionService
# Removed redesigned_gemini_prompts import - functionality integrated directly
from app.services.user_context_builder import UserContextBuilder
from app.services.question_validation_service import QuestionValidationService
from app.services.question_distribution_calculator import QuestionDistributionCalculator

logger = logging.getLogger(__name__)

# Configure Gemini API only if an API key is provided
if settings.GEMINI_API_KEY and settings.GEMINI_API_KEY != "your-gemini-api-key-here":
    try:
        genai.configure(api_key=settings.GEMINI_API_KEY)
        logger.info("Gemini API configured successfully")
    except Exception as e:
        logger.error(f"Failed to configure Gemini API: {str(e)}")
        # Fallback: leave unconfigured to force offline behavior
        pass
else:
    logger.warning("No valid Gemini API key provided, using fallback mode")


class GeminiService:
    """Service for interacting with Google Gemini API"""
    
    def __init__(self, db: Session):
        self.db = db
        self.fallback_service = FallbackQuestionService(db)
        # Integrated prompt functionality directly into this service
        self.context_builder = UserContextBuilder(db)
        self.validation_service = QuestionValidationService()
        self.distribution_calculator = QuestionDistributionCalculator()
        try:
            # Create model only when API is available
            if settings.GEMINI_API_KEY:
                # Use the working model first, fallback to others
                model_names = ['gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-1.0-pro', 'gemini-pro']
                self.model = None
                
                for model_name in model_names:
                    try:
                        self.model = genai.GenerativeModel(model_name)
                        logger.info(f"Gemini model '{model_name}' initialized successfully")
                        break
                    except Exception as model_error:
                        logger.warning(f"Failed to initialize model '{model_name}': {str(model_error)}")
                        continue
                
                if not self.model:
                    logger.error("All Gemini model attempts failed, using fallback mode")
            else:
                self.model = None
                logger.info("Gemini model not available, using fallback mode")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini model: {str(e)}")
            self.model = None
        
        self.cache = {}  # Simple in-memory cache
        self.cache_ttl = timedelta(hours=1)
    
    def _get_role_attribute(self, role_obj, attr_name: str, default_value=''):
        """Helper method to get attribute from either dictionary or Pydantic model"""
        if role_obj is None:
            return default_value
        if hasattr(role_obj, attr_name):
            # Pydantic model case
            return getattr(role_obj, attr_name, default_value)
        else:
            # Dictionary case
            return role_obj.get(attr_name, default_value)
    
    def generate_questions_with_rich_context(
        self,
        user_id: int,
        session_data: Dict[str, Any],
        role_hierarchy: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Generate questions using redesigned prompts with comprehensive user context
        
        Args:
            user_id: ID of the user
            session_data: Session configuration data
            role_hierarchy: Optional hierarchical role data
            
        Returns:
            List of generated questions with rich context
        """
        
        try:
            logger.info(f"Generating questions with rich context for user {user_id}")
            
            # Validate Gemini API availability
            if not self.model or not settings.GEMINI_API_KEY or settings.GEMINI_API_KEY == "your-gemini-api-key-here":
                logger.error("Gemini API not configured properly")
                raise RuntimeError("Gemini API is required for rich context question generation")
            
            # Build comprehensive user context
            complete_context = self.context_builder.build_complete_context(
                user_id=user_id,
                session_data=session_data,
                role_hierarchy=role_hierarchy
            )
            
            logger.info(f"Built complete context with {len(complete_context)} components")
            
            # Generate prompt using integrated prompt methods
            prompt = self.build_technical_question_prompt(complete_context)
            
            logger.info(f"Generated comprehensive prompt with {len(prompt)} characters")
            
            # Enhanced generation config for rich context
            generation_config = {
                "temperature": 0.7,  # Balanced creativity and consistency
                "top_p": 0.9,
                "top_k": 40,
                "max_output_tokens": 4096,  # Increased for detailed responses
            }
            
            # Generate questions using Gemini API
            response = self.model.generate_content(
                prompt,
                generation_config=generation_config
            )
            
            # Extract response text
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            if not response_text:
                logger.error("Empty response from Gemini API")
                raise RuntimeError("Empty response from Gemini API")
            
            # Parse questions from response
            questions_data = self._parse_questions_response(response_text)
            
            if not questions_data or len(questions_data) == 0:
                logger.error("No questions parsed from Gemini response")
                raise RuntimeError("Failed to parse questions from Gemini response")
            
            # Validate questions against user context using the validation service
            validation_results = self.validation_service.validate_questions_against_context(
                questions_data, complete_context, use_ai_validation=True, gemini_service=self
            )
            
            validated_questions = validation_results['validated_questions']
            
            # Validate and enforce question distribution
            distribution_results = self._validate_and_enforce_distribution(
                validated_questions, complete_context
            )
            
            if distribution_results['needs_adjustment']:
                logger.info("Question distribution needs adjustment, regenerating specific types")
                validated_questions = self._adjust_question_distribution(
                    validated_questions, distribution_results, complete_context
                )
            
            if len(validated_questions) < len(questions_data):
                logger.info(f"Filtered {len(questions_data) - len(validated_questions)} questions during validation")
            
            if not validated_questions:
                logger.warning("No questions passed validation, attempting regeneration...")
                # Use validation service to regenerate failed questions
                rejected_questions = validation_results.get('rejected_questions', [])
                regenerated_questions = self.validation_service.regenerate_failed_questions(
                    rejected_questions, complete_context, self
                )
                
                if regenerated_questions:
                    # Validate regenerated questions
                    regen_validation = self.validation_service.validate_questions_against_context(
                        regenerated_questions, complete_context, use_ai_validation=False
                    )
                    validated_questions = regen_validation['validated_questions']
                
                if not validated_questions:
                    logger.error("Regeneration failed, using fallback questions")
                    return self._get_fallback_questions_for_context(complete_context)
            
            logger.info(f"Successfully generated {len(validated_questions)} validated questions with rich context")
            
            # Store questions in database
            self._store_questions(
                validated_questions, 
                complete_context['user_profile']['role_hierarchy']['main_role'],
                complete_context['session_context']['current_difficulty'],
                'technical'
            )
            
            return validated_questions
            
        except Exception as e:
            logger.error(f"Error generating questions with rich context: {str(e)}")
            # Fallback to original method if rich context fails
            logger.warning("Falling back to original question generation method")
            return self.generate_questions(
                role=session_data.get('target_role', 'Software Developer'),
                difficulty=session_data.get('difficulty', 'medium'),
                question_type='mixed',
                count=session_data.get('question_count', 5),
                context=role_hierarchy
            )
    
    def generate_contextual_followup_with_rich_context(
        self,
        user_id: int,
        session_data: Dict[str, Any],
        previous_qa: Dict[str, Any],
        role_hierarchy: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Generate contextual follow-up questions using redesigned prompts
        
        Args:
            user_id: ID of the user
            session_data: Session configuration data
            previous_qa: Previous question and answer data
            role_hierarchy: Optional hierarchical role data
            
        Returns:
            List of contextual follow-up questions
        """
        
        try:
            logger.info(f"Generating contextual follow-up with rich context for user {user_id}")
            
            # Validate Gemini API availability
            if not self.model:
                raise RuntimeError("Gemini API not available")
            
            # Build comprehensive user context
            complete_context = self.context_builder.build_complete_context(
                user_id=user_id,
                session_data=session_data,
                role_hierarchy=role_hierarchy
            )
            
            # Generate follow-up prompt
            prompt = self.build_contextual_followup_prompt(
                complete_context, previous_qa
            )
            
            # Generate follow-up questions
            generation_config = {
                "temperature": 0.8,  # Higher creativity for follow-ups
                "top_p": 0.9,
                "top_k": 40,
                "max_output_tokens": 2048,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            # Extract and parse response
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            followup_questions = self._parse_questions_response(response_text)
            
            if followup_questions:
                logger.info(f"Generated {len(followup_questions)} contextual follow-up questions")
                return followup_questions
            else:
                logger.warning("No follow-up questions generated, creating fallback")
                return self._create_intelligent_followup(
                    previous_qa.get('question', ''),
                    previous_qa.get('answer', ''),
                    complete_context['user_profile']['role_hierarchy']['main_role'],
                    role_hierarchy
                )
                
        except Exception as e:
            logger.error(f"Error generating contextual follow-up: {str(e)}")
            # Fallback to original method
            return self.generate_contextual_questions(
                role=session_data.get('target_role', 'Software Developer'),
                previous_question=previous_qa.get('question', ''),
                user_answer=previous_qa.get('answer', ''),
                difficulty=session_data.get('difficulty', 'medium'),
                count=2,
                role_context=role_hierarchy
            )
    
    def evaluate_answer_with_rich_context(
        self,
        user_id: int,
        session_data: Dict[str, Any],
        qa_data: Dict[str, Any],
        role_hierarchy: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate user answer using redesigned prompts with rich context
        
        Args:
            user_id: ID of the user
            session_data: Session configuration data
            qa_data: Question and answer data
            role_hierarchy: Optional hierarchical role data
            
        Returns:
            Comprehensive evaluation results
        """
        
        try:
            logger.info(f"Evaluating answer with rich context for user {user_id}")
            
            # Validate Gemini API availability
            if not self.model:
                raise RuntimeError("Gemini API not available")
            
            # Build comprehensive user context
            complete_context = self.context_builder.build_complete_context(
                user_id=user_id,
                session_data=session_data,
                role_hierarchy=role_hierarchy
            )
            
            # Generate evaluation prompt
            prompt = self.build_answer_evaluation_prompt(
                complete_context, qa_data
            )
            
            # Evaluate answer
            generation_config = {
                "temperature": 0.3,  # Lower temperature for consistent evaluation
                "top_p": 0.8,
                "max_output_tokens": 2048,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            # Extract and parse response
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            evaluation = self._parse_evaluation_response(response_text)
            
            if evaluation:
                logger.info("Successfully evaluated answer with rich context")
                return evaluation
            else:
                logger.warning("Failed to parse evaluation, using fallback")
                return self._get_fallback_evaluation()
                
        except Exception as e:
            logger.error(f"Error evaluating answer with rich context: {str(e)}")
            # Fallback to original method
            return self.evaluate_answer(
                question=qa_data.get('question', ''),
                answer=qa_data.get('answer', ''),
                context=role_hierarchy or {}
            )
    
    def _validate_questions_with_context(
        self, 
        questions: List[Dict[str, Any]], 
        context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Validate generated questions against user context using redesigned validation
        
        Args:
            questions: List of generated questions
            context: Complete user context
            
        Returns:
            List of validated questions
        """
        
        try:
            # Use integrated validation prompt
            validation_prompt = self.build_validation_prompt(questions, context)
            
            generation_config = {
                "temperature": 0.2,  # Low temperature for consistent validation
                "top_p": 0.8,
                "max_output_tokens": 3072,
            }
            
            response = self.model.generate_content(validation_prompt, generation_config=generation_config)
            
            # Extract response text
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            # Parse validation results
            validation_results = self._parse_validation_response(response_text)
            
            if validation_results and 'validated_questions' in validation_results:
                validated = validation_results['validated_questions']
                logger.info(f"Validation completed: {len(validated)} questions passed validation")
                return validated
            else:
                logger.warning("Validation parsing failed, using basic validation")
                return self._basic_question_validation(questions, context)
                
        except Exception as e:
            logger.error(f"Error in context-based validation: {str(e)}")
            return self._basic_question_validation(questions, context)
    
    def _regenerate_questions_with_feedback(
        self, 
        context: Dict[str, Any], 
        failed_questions: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Regenerate questions with feedback from failed validation
        
        Args:
            context: Complete user context
            failed_questions: Questions that failed validation
            
        Returns:
            List of regenerated questions
        """
        
        try:
            logger.info("Regenerating questions with validation feedback")
            
            # Build feedback-enhanced prompt
            feedback_prompt = self._build_regeneration_prompt_with_feedback(context, failed_questions)
            
            generation_config = {
                "temperature": 0.6,  # Moderate creativity with more focus
                "top_p": 0.8,
                "top_k": 30,
                "max_output_tokens": 3072,
            }
            
            response = self.model.generate_content(feedback_prompt, generation_config=generation_config)
            
            # Extract and parse response
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            regenerated_questions = self._parse_questions_response(response_text)
            
            if regenerated_questions:
                logger.info(f"Successfully regenerated {len(regenerated_questions)} questions")
                return regenerated_questions
            else:
                logger.warning("Regeneration failed, using fallback questions")
                return self._get_fallback_questions_for_context(context)
                
        except Exception as e:
            logger.error(f"Error regenerating questions: {str(e)}")
            return self._get_fallback_questions_for_context(context)
    
    def _basic_question_validation(
        self, 
        questions: List[Dict[str, Any]], 
        context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Basic validation of questions against context
        
        Args:
            questions: List of questions to validate
            context: User context for validation
            
        Returns:
            List of validated questions
        """
        
        validated = []
        user_profile = context.get('user_profile', {})
        role_hierarchy = user_profile.get('role_hierarchy', {})
        tech_stacks = user_profile.get('tech_stacks', [])
        
        # Handle both dictionary and Pydantic model cases for role_hierarchy
        main_role = self._get_role_attribute(role_hierarchy, 'main_role', '').lower()
        specialization = self._get_role_attribute(role_hierarchy, 'specialization', '').lower()
        
        for question in questions:
            question_text = question.get('question', '').lower()
            
            # Basic relevance checks
            is_relevant = False
            
            # Check role relevance
            if main_role and (main_role in question_text or any(word in question_text for word in main_role.split())):
                is_relevant = True
            
            # Check specialization relevance
            if specialization and (specialization in question_text or any(word in question_text for word in specialization.split())):
                is_relevant = True
            
            # Check tech stack relevance
            if tech_stacks and any(tech.lower() in question_text for tech in tech_stacks):
                is_relevant = True
            
            # Check for generic programming terms (fallback)
            generic_terms = ['code', 'program', 'algorithm', 'system', 'design', 'implement', 'develop']
            if any(term in question_text for term in generic_terms):
                is_relevant = True
            
            if is_relevant:
                validated.append(question)
        
        logger.info(f"Basic validation: {len(validated)}/{len(questions)} questions passed")
        return validated
    
    def _parse_validation_response(self, response_text: str) -> Optional[Dict[str, Any]]:
        """Parse validation response from Gemini API"""
        
        try:
            # Try to extract JSON from response
            import json
            import re
            
            # Look for JSON structure in response
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                return json.loads(json_str)
            
            return None
            
        except Exception as e:
            logger.error(f"Error parsing validation response: {str(e)}")
            return None
    
    def _build_regeneration_prompt_with_feedback(
        self, 
        context: Dict[str, Any], 
        failed_questions: List[Dict[str, Any]]
    ) -> str:
        """Build prompt for regenerating questions with feedback"""
        
        user_profile = context['user_profile']
        session_context = context['session_context']
        
        failed_examples = "\n".join([
            f"- {q.get('question', 'N/A')}" for q in failed_questions[:3]
        ])
        
        return f"""
The previous questions failed validation. Generate better questions that address these issues:

FAILED QUESTIONS EXAMPLES:
{failed_examples}

COMMON ISSUES TO AVOID:
- Generic questions that could apply to any role
- Questions unrelated to {user_profile['role_hierarchy']['specialization']}
- Questions inappropriate for {session_context['current_difficulty']} difficulty
- Questions not testing role-specific competencies

REGENERATION REQUIREMENTS:
- Focus specifically on {user_profile['role_hierarchy']['main_role']} responsibilities
- Test {user_profile['role_hierarchy']['specialization']} expertise
- Match {session_context['current_difficulty']} difficulty precisely
- Use technologies from: {', '.join(user_profile['tech_stacks'][:5]) if user_profile['tech_stacks'] else 'general tech stack'}

Generate {session_context['question_count']} improved questions following the original format.
"""
    
    def _get_fallback_questions_for_context(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Get fallback questions based on context"""
        
        user_profile = context['user_profile']
        session_context = context['session_context']
        
        # Use fallback service with context
        return self.fallback_service.get_questions_by_role(
            role=user_profile['role_hierarchy']['main_role'],
            difficulty=session_context['current_difficulty'],
            count=session_context['question_count']
        )
    
    def _validate_and_enforce_distribution(
        self, 
        questions: List[Dict[str, Any]], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Validate question distribution and determine if adjustment is needed
        
        Args:
            questions: List of validated questions
            context: Complete user context
            
        Returns:
            Distribution validation results
        """
        
        try:
            session_context = context['session_context']
            question_count = session_context['question_count']
            session_type = session_context.get('session_type', 'main')
            
            # Calculate expected distribution
            expected_distribution = self.distribution_calculator.calculate_distribution(
                total_questions=question_count,
                session_type=session_type
            )
            
            # Validate current distribution
            validation_results = self.distribution_calculator.validate_distribution(
                questions, expected_distribution
            )
            
            # Determine if adjustment is needed
            needs_adjustment = not validation_results['is_valid'] or validation_results['validation_score'] < 80
            
            return {
                'needs_adjustment': needs_adjustment,
                'validation_results': validation_results,
                'expected_distribution': expected_distribution,
                'adjustment_needed': self.distribution_calculator.adjust_distribution_for_regeneration(
                    questions, expected_distribution
                ) if needs_adjustment else {}
            }
            
        except Exception as e:
            logger.error(f"Error validating distribution: {str(e)}")
            return {
                'needs_adjustment': False,
                'validation_results': {},
                'expected_distribution': {},
                'adjustment_needed': {}
            }
    
    def _adjust_question_distribution(
        self, 
        current_questions: List[Dict[str, Any]], 
        distribution_results: Dict[str, Any],
        context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Adjust question distribution by regenerating specific question types
        
        Args:
            current_questions: Current list of questions
            distribution_results: Distribution validation results
            context: Complete user context
            
        Returns:
            Adjusted list of questions with proper distribution
        """
        
        try:
            adjustment_needed = distribution_results['adjustment_needed']
            
            # If no adjustment needed, return current questions
            if not any(count > 0 for count in adjustment_needed.values()):
                return current_questions
            
            logger.info(f"Adjusting distribution: {adjustment_needed}")
            
            # Generate additional questions for types that are lacking
            additional_questions = []
            
            for question_type, needed_count in adjustment_needed.items():
                if needed_count > 0:
                    logger.info(f"Generating {needed_count} additional {question_type} questions")
                    
                    # Generate specific type of questions
                    type_questions = self._generate_specific_question_type(
                        question_type, needed_count, context
                    )
                    
                    additional_questions.extend(type_questions)
            
            # Combine current questions with additional ones
            # Remove excess questions if we have too many of certain types
            adjusted_questions = self._balance_question_types(
                current_questions + additional_questions,
                distribution_results['expected_distribution']
            )
            
            logger.info(f"Distribution adjusted: {len(adjusted_questions)} total questions")
            return adjusted_questions
            
        except Exception as e:
            logger.error(f"Error adjusting distribution: {str(e)}")
            return current_questions
    
    def _generate_specific_question_type(
        self, 
        question_type: str, 
        count: int, 
        context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Generate questions of a specific type
        
        Args:
            question_type: Type of questions to generate (theory, coding, aptitude)
            count: Number of questions to generate
            context: Complete user context
            
        Returns:
            List of generated questions of the specified type
        """
        
        try:
            user_profile = context['user_profile']
            session_context = context['session_context']
            role_hierarchy = user_profile['role_hierarchy']
            difficulty = session_context['current_difficulty']
            
            # Get examples for this question type
            examples = self.distribution_calculator.get_question_examples_for_role(
                role_hierarchy, difficulty, question_type
            )
            
            # Build a focused prompt for this question type
            focused_prompt = f"""
Generate exactly {count} {question_type} questions for a {role_hierarchy['main_role']} - {role_hierarchy['sub_role']} - {role_hierarchy['specialization']} candidate.

QUESTION TYPE FOCUS: {question_type.upper()}
DIFFICULTY LEVEL: {difficulty}
REQUIRED COUNT: {count}

EXAMPLES FOR THIS TYPE:
{chr(10).join(f"- {example}" for example in examples[:3])}

REQUIREMENTS:
- All questions must be {question_type} type only
- Match {difficulty} difficulty level
- Be relevant to {role_hierarchy['specialization']}
- Test different aspects within this question type

FORMAT: Return as JSON array with question, category, duration, key_points fields.
"""
            
            # Generate questions using Gemini
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.9,
                "max_output_tokens": 2048,
            }
            
            response = self.model.generate_content(focused_prompt, generation_config=generation_config)
            
            # Extract response text
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            # Parse questions
            questions = self._parse_questions_response(response_text)
            
            # Ensure questions are marked with correct category
            for question in questions:
                question['category'] = question_type
            
            logger.info(f"Generated {len(questions)} {question_type} questions")
            return questions[:count]  # Limit to requested count
            
        except Exception as e:
            logger.error(f"Error generating {question_type} questions: {str(e)}")
            return []
    
    def _balance_question_types(
        self, 
        questions: List[Dict[str, Any]], 
        target_distribution: Dict[str, int]
    ) -> List[Dict[str, Any]]:
        """
        Balance question types to match target distribution
        
        Args:
            questions: List of all questions
            target_distribution: Target distribution counts
            
        Returns:
            Balanced list of questions
        """
        
        try:
            # Group questions by type
            questions_by_type = {
                'theory': [],
                'coding': [],
                'aptitude': []
            }
            
            for question in questions:
                category = question.get('category', '').lower()
                
                # Map categories to distribution types
                if category in ['theory', 'theoretical']:
                    questions_by_type['theory'].append(question)
                elif category in ['coding', 'technical', 'programming']:
                    questions_by_type['coding'].append(question)
                elif category in ['aptitude', 'problem-solving', 'analytical']:
                    questions_by_type['aptitude'].append(question)
                else:
                    # Default classification based on content
                    question_text = question.get('question', '').lower()
                    if any(word in question_text for word in ['explain', 'define', 'what is']):
                        questions_by_type['theory'].append(question)
                    elif any(word in question_text for word in ['implement', 'write', 'code']):
                        questions_by_type['coding'].append(question)
                    else:
                        questions_by_type['aptitude'].append(question)
            
            # Select questions according to target distribution
            balanced_questions = []
            
            for question_type, target_count in target_distribution.items():
                available_questions = questions_by_type[question_type]
                
                # Take up to target_count questions of this type
                selected_questions = available_questions[:target_count]
                balanced_questions.extend(selected_questions)
                
                logger.debug(f"Selected {len(selected_questions)}/{target_count} {question_type} questions")
            
            logger.info(f"Balanced questions: {len(balanced_questions)} total")
            return balanced_questions
            
        except Exception as e:
            logger.error(f"Error balancing question types: {str(e)}")
            return questions
    
    def generate_questions(
        self, 
        role: str, 
        difficulty: str = "intermediate", 
        question_type: str = "mixed",
        count: int = 5,
        context: Dict[str, Any] = None,
        previous_answers: List[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """Generate interview questions using Gemini API with context awareness"""
        
        # Build cache key including context for better caching
        context_key = ""
        if context or previous_answers:
            context_key = f"_ctx_{hash(str(context or {}))}_ans_{hash(str(previous_answers or []))}"
        cache_key = f"{role}_{difficulty}_{question_type}_{count}{context_key}"
        
        # Skip cache for contextual questions to ensure freshness
        if not context and not previous_answers and self._is_cached(cache_key):
            logger.info(f"Returning cached questions for {cache_key}")
            return self.cache[cache_key]["data"]
        
        # Force Gemini API usage - only fallback if API completely fails
        if not self.model or not settings.GEMINI_API_KEY or settings.GEMINI_API_KEY == "your-gemini-api-key-here":
            logger.error("Gemini API not configured properly")
            raise RuntimeError("Gemini API is required for question generation")
        
        try:
            logger.info(f"Generating questions using Gemini API for {role} with context: {bool(context or previous_answers)}")
            
            # For technical questions with role context, use specialized technical generation
            if question_type == "technical" and context and context.get('main_role'):
                logger.info("Using specialized technical question generation")
                technical_questions = self.generate_technical_questions(
                    context, difficulty, count, question_focus="mixed"
                )
                if technical_questions and len(technical_questions) >= count:
                    logger.info(f"Successfully generated {len(technical_questions)} specialized technical questions")
                    # Store questions in database
                    self._store_questions(technical_questions, role, difficulty, question_type)
                    return technical_questions[:count]
                else:
                    logger.warning("Specialized technical generation insufficient, falling back to general generation")
            
            # Build contextual prompt for general question generation
            prompt = self._build_contextual_question_prompt(
                role, difficulty, question_type, count, context, previous_answers
            )
            
            # Enhanced generation config for better contextual responses
            generation_config = {
                "temperature": 0.8,  # Higher for more creative contextual questions
                "top_p": 0.9,
                "top_k": 40,
                "max_output_tokens": 3072,
            }
            
            response = self.model.generate_content(
                prompt,
                generation_config=generation_config
            )
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            questions_data = self._parse_questions_response(response_text)
            
            if questions_data and len(questions_data) > 0:
                # Validate question relevance against role requirements
                if context:
                    validated_questions = self._validate_question_relevance(questions_data, context)
                    if len(validated_questions) < len(questions_data):
                        logger.info(f"Filtered {len(questions_data) - len(validated_questions)} irrelevant questions")
                    questions_data = validated_questions
                
                if questions_data:
                    logger.info(f"Successfully generated {len(questions_data)} role-specific questions via Gemini API")
                    
                    # Only cache non-contextual questions
                    if not context and not previous_answers:
                        self._cache_data(cache_key, questions_data)
                    
                    # Store questions in database (with duplicate prevention)
                    self._store_questions(questions_data, role, difficulty, question_type)
                    
                    return questions_data
            else:
                logger.error("Gemini API returned empty questions")
                raise RuntimeError("Failed to generate questions from Gemini API")
                
        except Exception as e:
            logger.error(f"Error generating questions with Gemini API: {str(e)}")
            logger.error(f"Error type: {type(e).__name__}")
            if hasattr(e, 'response'):
                logger.error(f"API Response: {e.response}")
            
            # Only use fallback for non-contextual questions as last resort
            if not context and not previous_answers:
                logger.warning("Using fallback questions as last resort")
                fallback_questions = self._get_fallback_questions(role, difficulty, question_type, count)
                self._cache_data(cache_key, fallback_questions)
                return fallback_questions
            else:
                # For contextual questions, we must have AI generation
                raise RuntimeError(f"Failed to generate contextual questions: {str(e)}")
    
    def generate_contextual_questions(
        self,
        role: str,
        previous_question: str,
        user_answer: str,
        difficulty: str = "intermediate",
        count: int = 2,
        role_context: Dict[str, Any] = None,
        performance_history: List[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """Generate enhanced contextual follow-up questions with difficulty progression"""
        
        try:
            logger.info(f"Generating enhanced contextual follow-up questions for {role}")
            
            # Analyze user performance to adjust difficulty
            adjusted_difficulty = self._analyze_performance_and_adjust_difficulty(
                user_answer, difficulty, performance_history
            )
            
            # Build enhanced contextual prompt
            prompt = self._build_enhanced_followup_question_prompt(
                role, previous_question, user_answer, adjusted_difficulty, count, role_context
            )
            
            generation_config = {
                "temperature": 0.9,  # High creativity for follow-ups
                "top_p": 0.9,
                "top_k": 40,
                "max_output_tokens": 2048,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            questions_data = self._parse_questions_response(response_text)
            
            if questions_data and len(questions_data) > 0:
                # Validate contextual relevance
                validated_questions = self._validate_contextual_relevance(
                    questions_data, previous_question, user_answer, role_context
                )
                
                if validated_questions:
                    logger.info(f"Generated {len(validated_questions)} contextual follow-up questions")
                    return validated_questions
            
            logger.warning("No valid contextual questions generated, creating intelligent follow-up")
            return self._create_intelligent_followup(previous_question, user_answer, role, role_context)
                
        except Exception as e:
            logger.error(f"Error generating contextual questions: {str(e)}")
            return self._create_intelligent_followup(previous_question, user_answer, role, role_context)
    
    def evaluate_answer(
        self, 
        question: str, 
        answer: str, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Evaluate user's answer using Gemini API"""
        
        try:
            prompt = self._build_evaluation_prompt(question, answer, context)
            
            if not self.model:
                raise RuntimeError("Gemini model not available")
            
            generation_config = {
                "temperature": 0.3,
                "top_p": 0.8,
                "max_output_tokens": 1024,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            evaluation = self._parse_evaluation_response(response_text)
            
            return evaluation
            
        except Exception as e:
            logger.error(f"Error evaluating answer: {str(e)}")
            logger.error(f"Error type: {type(e).__name__}")
            return self._get_fallback_evaluation()
    
    def generate_feedback(
        self, 
        performance_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate personalized feedback based on performance data"""
        
        try:
            prompt = self._build_feedback_prompt(performance_data)
            
            if not self.model:
                raise RuntimeError("Gemini model not available")
            response = self.model.generate_content(prompt)
            feedback = self._parse_feedback_response(getattr(response, 'text', '') or '')
            
            return feedback
            
        except Exception as e:
            logger.error(f"Error generating feedback: {str(e)}")
            return self._get_fallback_feedback()
    
    def generate_follow_up_questions(
        self, 
        original_question: str, 
        user_answer: str, 
        context: Dict[str, Any]
    ) -> List[str]:
        """Generate follow-up questions based on user's answer"""
        
        try:
            prompt = self._build_followup_prompt(original_question, user_answer, context)
            
            if not self.model:
                raise RuntimeError("Gemini model not available")
            response = self.model.generate_content(prompt)
            follow_ups = self._parse_followup_response(getattr(response, 'text', '') or '')
            
            return follow_ups
            
        except Exception as e:
            logger.error(f"Error generating follow-up questions: {str(e)}")
            return []
    
    def _build_question_prompt(
        self, 
        role: str, 
        difficulty: str, 
        question_type: str, 
        count: int
    ) -> str:
        """Build prompt for question generation"""
        
        type_instructions = {
            "behavioral": "behavioral and situational questions that assess soft skills, teamwork, and problem-solving approach",
            "technical": "technical questions that test domain-specific knowledge and skills",
            "mixed": "a mix of behavioral, technical, and situational questions"
        }
        
        difficulty_instructions = {
            "beginner": "entry-level questions suitable for candidates with 0-2 years of experience",
            "intermediate": "mid-level questions for candidates with 2-5 years of experience", 
            "advanced": "senior-level questions for experienced candidates with 5+ years"
        }
        
        prompt = f"""
        You are an expert interview coach and recruiter. Generate {count} realistic interview questions for a {role} position.

        Requirements:
        - Difficulty level: {difficulty_instructions.get(difficulty, 'intermediate')}
        - Question type: {type_instructions.get(question_type, 'mixed')}
        - Questions should be realistic and commonly asked in actual interviews
        - Include a mix of open-ended and specific questions
        - Questions should be relevant to current industry standards and practices

        For each question, provide:
        1. The question text
        2. Question category (behavioral, technical, situational, etc.)
        3. Expected answer duration in minutes
        4. Key points that a good answer should cover

        Format your response as a JSON array with this structure:
        [
            {{
                "question": "Question text here",
                "category": "behavioral|technical|situational",
                "duration": 3,
                "key_points": ["point 1", "point 2", "point 3"]
            }}
        ]

        Role: {role}
        Generate questions now:
        """
        
        return prompt
    
    def _build_contextual_question_prompt(
        self,
        role: str,
        difficulty: str,
        question_type: str,
        count: int,
        context: Dict[str, Any] = None,
        previous_answers: List[Dict[str, Any]] = None
    ) -> str:
        """Build enhanced contextual prompt using hierarchical role data and previous answer analysis"""
        
        type_instructions = {
            "behavioral": "behavioral and situational questions that assess soft skills, teamwork, and problem-solving approach",
            "technical": "technical questions that test domain-specific knowledge and skills",
            "mixed": "a mix of behavioral, technical, and situational questions"
        }
        
        difficulty_instructions = {
            "beginner": "entry-level questions suitable for candidates with 0-2 years of experience",
            "intermediate": "mid-level questions for candidates with 2-5 years of experience", 
            "advanced": "senior-level questions for experienced candidates with 5+ years"
        }
        
        # Build enhanced hierarchical role context with technical competency mapping
        role_context = ""
        technical_competencies = []
        role_specific_scenarios = []
        
        if context:
            # Handle both dictionary and Pydantic model cases
            main_role = self._get_role_attribute(context, 'main_role', '')
            sub_role = self._get_role_attribute(context, 'sub_role', '')
            specialization = self._get_role_attribute(context, 'specialization', '')
            tech_stacks = self._get_role_attribute(context, 'tech_stacks', [])
            question_tags = self._get_role_attribute(context, 'question_tags', [])
            
            # Map role to technical competencies and scenarios
            technical_competencies = self._get_technical_competencies(main_role, sub_role, specialization)
            role_specific_scenarios = self._get_role_scenarios(main_role)
            
            role_context = f"\n\nENHANCED ROLE HIERARCHY CONTEXT:\n"
            role_context += f"Main Role: {main_role}\n"
            if sub_role:
                role_context += f"Sub Role: {sub_role}\n"
            if specialization:
                role_context += f"Specialization: {specialization}\n"
            if tech_stacks:
                role_context += f"Required Technologies: {', '.join(tech_stacks[:5])}\n"
            if question_tags:
                role_context += f"Key Competency Areas: {', '.join(question_tags[:5])}\n"
            if technical_competencies:
                role_context += f"Technical Competencies to Assess: {', '.join(technical_competencies[:5])}\n"
            if role_specific_scenarios:
                role_context += f"Role-Specific Scenarios: {', '.join(role_specific_scenarios[:3])}\n"
            
            role_context += f"""
            
            HIERARCHICAL ROLE-SPECIFIC REQUIREMENTS:
            - Focus specifically on {main_role} core responsibilities and daily challenges
            - Include {sub_role} specialized scenarios and domain knowledge
            - Test {specialization} advanced expertise and best practices
            - Reference specific technologies: {', '.join(tech_stacks[:3]) if tech_stacks else 'industry-standard tools'}
            - Assess technical competencies: {', '.join(technical_competencies[:3]) if technical_competencies else 'core technical skills'}
            - Create questions that differentiate between skill levels in this exact role hierarchy
            - Use role-specific scenarios: {', '.join(role_specific_scenarios[:2]) if role_specific_scenarios else 'general professional scenarios'}
            """
        
        # Enhanced previous answers analysis with follow-up generation
        context_section = ""
        covered_topics = set()
        answer_insights = []
        performance_indicators = []
        
        if previous_answers:
            context_section = "\n\nENHANCED PREVIOUS INTERVIEW CONTEXT:\n"
            
            # Analyze each previous Q&A for deeper insights
            for i, qa in enumerate(previous_answers[-3:], 1):  # Use last 3 Q&As for context
                question = qa.get('question', '')
                answer = qa.get('answer', '')
                score = qa.get('score', {})
                
                context_section += f"Q{i}: {question}\n"
                context_section += f"A{i}: {answer}\n"
                
                # Extract performance insights
                if score:
                    overall_score = score.get('overall_score', 70)
                    performance_indicators.append(overall_score)
                    context_section += f"Performance Score: {overall_score}/100\n"
                
                context_section += "\n"
                
                # Extract topics and insights for follow-up generation
                covered_topics.update(self._extract_question_topics(question))
                insights = self._extract_answer_insights(answer, question)
                answer_insights.extend(insights)
            
            # Calculate performance trend for difficulty adjustment
            avg_performance = sum(performance_indicators) / len(performance_indicators) if performance_indicators else 70
            
            context_section += f"""
            CONTEXTUAL FOLLOW-UP REQUIREMENTS:
            - Generate questions that build naturally on the candidate's previous responses
            - Dive deeper into specific examples, projects, or technologies they mentioned: {', '.join(answer_insights[:5]) if answer_insights else 'None identified'}
            - Ask for concrete details about their experience and decision-making process
            - Explore the technical depth of their knowledge in areas they referenced
            - Create progressive difficulty based on demonstrated competency (Current avg: {avg_performance:.1f}/100)
            - Follow up on interesting points: {', '.join(answer_insights[:3]) if answer_insights else 'None'}
            
            AVOID REPETITION: Do not ask about these already covered topics: {', '.join(covered_topics) if covered_topics else 'None'}
            """
        
        # Enhanced anti-repetition and relevance validation
        anti_repetition_section = f"""
        
        ENHANCED ANTI-REPETITION & RELEVANCE REQUIREMENTS:
        - Ensure each question is unique and tests different aspects of the role hierarchy
        - Avoid generic questions that could apply to any position
        - Do not repeat similar question patterns or structures
        - Focus on role-specific scenarios and challenges from the hierarchical context
        - Each question should reveal different competencies or experiences
        - Validate questions against role requirements before including them
        - Ensure technical questions match the specific tech stack and competencies
        """
        
        # Add enhanced role-specific question validation criteria
        validation_criteria = self._get_role_validation_criteria(context.get('main_role', role) if context else role)
        
        prompt = f"""
        You are an expert technical interviewer conducting a specialized interview for a {role} position with hierarchical role context.
        
        GENERATION REQUIREMENTS:
        - Difficulty level: {difficulty_instructions.get(difficulty, 'intermediate')}
        - Question type: {type_instructions.get(question_type, 'mixed')}
        - Generate exactly {count} unique, role-hierarchy-specific questions
        - Each question must be directly relevant to the specific role hierarchy and competencies
        - Avoid generic questions that could apply to any job
        - Use hierarchical role data to create highly targeted questions
        
        {role_context}
        {context_section}
        {anti_repetition_section}
        
        ENHANCED QUESTION VALIDATION CRITERIA:
        {validation_criteria}
        
        For each question, provide:
        1. The question text (specific to the role hierarchy and technical competencies)
        2. Question category (behavioral, technical, situational, problem-solving, system-design)
        3. Expected answer duration in minutes (2-5 minutes)
        4. Key technical/behavioral points that demonstrate competency
        5. Specific role relevance (why this question matters for this exact position)
        6. Hierarchical context usage (how this leverages the role hierarchy data)
        
        Format your response as a JSON array:
        [
            {{
                "question": "Specific, role-hierarchy-targeted question text here",
                "category": "behavioral|technical|situational|problem-solving|system-design",
                "duration": 3,
                "key_points": ["specific competency 1", "technical skill 2", "behavioral indicator 3"],
                "role_relevance": "Why this question is essential for this specific role hierarchy",
                "hierarchical_context": "How this question leverages the main_role/sub_role/specialization data"
            }}
        ]
        
        Generate hierarchical role-optimized questions now:
        """
        
        return prompt
    
    def _build_followup_question_prompt(
        self,
        role: str,
        previous_question: str,
        user_answer: str,
        difficulty: str,
        count: int
    ) -> str:
        """Build prompt for generating follow-up questions based on specific answer"""
        
        prompt = f"""
        You are an expert interviewer conducting a {role} interview. Based on the candidate's answer, generate {count} natural follow-up questions.
        
        PREVIOUS QUESTION: {previous_question}
        
        CANDIDATE'S ANSWER: {user_answer}
        
        Generate follow-up questions that:
        - Dive deeper into specific points they mentioned
        - Ask for concrete examples or details
        - Explore the technical or behavioral aspects further
        - Feel like a natural conversation progression
        - Are appropriate for {difficulty} level candidates
        
        Examples of good follow-ups:
        - If they mention "projects" → "Tell me about one specific project you're most proud of"
        - If they mention "database work" → "What database technologies have you worked with?"
        - If they mention "team collaboration" → "Can you give me an example of how you handled a team conflict?"
        
        Format your response as a JSON array:
        [
            {{
                "question": "Follow-up question text here",
                "category": "behavioral|technical|situational",
                "duration": 3,
                "key_points": ["point 1", "point 2", "point 3"]
            }}
        ]
        
        Generate follow-up questions now:
        """
        
        return prompt
    
    def _build_enhanced_followup_question_prompt(
        self,
        role: str,
        previous_question: str,
        user_answer: str,
        difficulty: str,
        count: int,
        role_context: Dict[str, Any] = None
    ) -> str:
        """Build enhanced follow-up question prompt with hierarchical role context and performance analysis"""
        
        # Extract insights from the user's answer
        answer_insights = self._extract_answer_insights(user_answer, previous_question)
        
        # Build role context if available
        role_context_section = ""
        if role_context:
            # Handle both dictionary and Pydantic model cases
            main_role = self._get_role_attribute(role_context, 'main_role', '')
            sub_role = self._get_role_attribute(role_context, 'sub_role', '')
            specialization = self._get_role_attribute(role_context, 'specialization', '')
            tech_stacks = role_context.get('tech_stacks', [])
            
            role_context_section = f"""
            
            HIERARCHICAL ROLE CONTEXT:
            - Main Role: {main_role}
            - Sub Role: {sub_role}
            - Specialization: {specialization}
            - Tech Stack: {', '.join(tech_stacks[:3]) if tech_stacks else 'Not specified'}
            
            Use this context to generate role-specific follow-up questions that test deeper competencies.
            """
        
        # Build answer analysis section
        answer_analysis_section = ""
        if answer_insights:
            answer_analysis_section = f"""
            
            ANSWER ANALYSIS INSIGHTS:
            The candidate's response indicates: {', '.join(answer_insights)}
            
            Use these insights to generate targeted follow-up questions that explore:
            - Specific technologies or methodologies they mentioned
            - Projects or experiences they referenced
            - Challenges or problems they've faced
            - Their decision-making process and reasoning
            """
        
        difficulty_guidance = {
            "beginner": "Ask for basic examples and simple explanations of concepts they mentioned",
            "intermediate": "Probe for deeper understanding, trade-offs, and real-world application",
            "advanced": "Explore complex scenarios, architectural decisions, and leadership aspects"
        }
        
        prompt = f"""
        You are an expert technical interviewer conducting an in-depth {role} interview. Based on the candidate's answer, generate {count} intelligent follow-up questions that build naturally on their response.
        
        PREVIOUS QUESTION: {previous_question}
        
        CANDIDATE'S ANSWER: {user_answer}
        
        {role_context_section}
        {answer_analysis_section}
        
        ENHANCED FOLLOW-UP REQUIREMENTS:
        - Difficulty Level: {difficulty} - {difficulty_guidance.get(difficulty, 'Adjust complexity appropriately')}
        - Build directly on specific points from their answer
        - Ask for concrete examples, technical details, or decision rationales
        - Explore the depth of their knowledge in areas they mentioned
        - Test their understanding of trade-offs and best practices
        - Create questions that feel like a natural conversation progression
        - Ensure questions are role-specific and technically relevant
        
        FOLLOW-UP QUESTION STRATEGIES:
        1. Deep Dive: "You mentioned [specific point] - can you walk me through how you [specific action]?"
        2. Technical Depth: "What were the technical challenges with [technology/approach they mentioned]?"
        3. Decision Analysis: "How did you decide between [alternatives they might have considered]?"
        4. Scale/Complexity: "How would your approach change if [scale/complexity increased]?"
        5. Learning/Growth: "What did you learn from [experience they described]?"
        
        For each question, provide:
        1. Question text that builds on their specific answer
        2. Category (technical, behavioral, situational, problem-solving)
        3. Expected duration (3-5 minutes for follow-ups)
        4. Key points that a good answer should demonstrate
        5. Connection to their previous answer (what aspect you're exploring)
        
        Format your response as a JSON array:
        [
            {{
                "question": "Specific follow-up question based on their answer",
                "category": "technical|behavioral|situational|problem-solving",
                "duration": 4,
                "key_points": ["specific competency 1", "technical depth 2", "decision-making 3"],
                "answer_connection": "What aspect of their previous answer this explores"
            }}
        ]
        
        Generate intelligent follow-up questions now:
        """
        
        return prompt
    
    def _build_evaluation_prompt(
        self, 
        question: str, 
        answer: str, 
        context: Dict[str, Any]
    ) -> str:
        """Build prompt for answer evaluation"""
        
        user_role = context.get('role', 'job_seeker')
        experience_level = context.get('experience_level', 'intermediate')
        target_role = context.get('target_role', 'general')
        
        prompt = f"""
        You are an expert interview evaluator. Evaluate the following interview answer.

        Question: {question}
        
        Candidate's Answer: {answer}
        
        Context:
        - Candidate Role: {user_role}
        - Experience Level: {experience_level}
        - Target Position: {target_role}

        Evaluate the answer on these criteria:
        1. Content Quality (0-100): Relevance, completeness, accuracy
        2. Communication (0-100): Clarity, structure, professionalism
        3. Depth (0-100): Level of detail and insight
        4. Relevance (0-100): How well it addresses the question

        Provide:
        - Overall score (0-100)
        - Scores for each criterion
        - Specific strengths (2-3 points)
        - Areas for improvement (2-3 points)
        - Actionable suggestions for better answers

        Format as JSON:
        {{
            "overall_score": 85,
            "scores": {{
                "content_quality": 80,
                "communication": 90,
                "depth": 85,
                "relevance": 85
            }},
            "strengths": ["strength 1", "strength 2"],
            "improvements": ["improvement 1", "improvement 2"],
            "suggestions": ["suggestion 1", "suggestion 2"]
        }}
        """
        
        return prompt
    
    def _build_feedback_prompt(self, performance_data: Dict[str, Any]) -> str:
        """Build prompt for personalized feedback generation"""
        
        prompt = f"""
        You are an expert interview coach. Generate personalized feedback based on the candidate's overall performance.

        Performance Data:
        {json.dumps(performance_data, indent=2)}

        Generate comprehensive feedback including:
        1. Overall performance summary
        2. Key strengths to leverage
        3. Priority areas for improvement
        4. Specific action items and practice recommendations
        5. Motivational closing message

        Format as JSON:
        {{
            "summary": "Overall performance summary",
            "strengths": ["strength 1", "strength 2"],
            "improvements": ["area 1", "area 2"],
            "action_items": ["action 1", "action 2"],
            "motivation": "Encouraging message"
        }}
        """
        
        return prompt
    
    def _build_followup_prompt(
        self, 
        original_question: str, 
        user_answer: str, 
        context: Dict[str, Any]
    ) -> str:
        """Build prompt for follow-up question generation"""
        
        prompt = f"""
        You are an experienced interviewer. Based on the candidate's answer, generate 2-3 relevant follow-up questions.

        Original Question: {original_question}
        Candidate's Answer: {user_answer}
        
        Generate follow-up questions that:
        - Dig deeper into the candidate's response
        - Clarify any ambiguous points
        - Explore related scenarios or experiences
        - Test the depth of their knowledge/experience

        Format as JSON array:
        ["follow-up question 1", "follow-up question 2", "follow-up question 3"]
        """
        
        return prompt
    
    def _parse_questions_response(self, response_text: str) -> List[Dict[str, Any]]:
        """Parse Gemini response for questions"""
        try:
            # Clean the response text
            cleaned_text = response_text.strip()
            if cleaned_text.startswith('```json'):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith('```'):
                cleaned_text = cleaned_text[:-3]
            
            questions = json.loads(cleaned_text)
            
            # Validate and clean questions
            validated_questions = []
            for q in questions:
                if isinstance(q, dict) and 'question' in q:
                    validated_questions.append({
                        'question': q.get('question', ''),
                        'category': q.get('category', 'general'),
                        'duration': q.get('duration', 3),
                        'key_points': q.get('key_points', [])
                    })
            
            return validated_questions
            
        except Exception as e:
            logger.error(f"Error parsing questions response: {str(e)}")
            return []
    
    def _parse_evaluation_response(self, response_text: str) -> Dict[str, Any]:
        """Parse Gemini response for answer evaluation"""
        try:
            cleaned_text = response_text.strip()
            if cleaned_text.startswith('```json'):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith('```'):
                cleaned_text = cleaned_text[:-3]
            
            evaluation = json.loads(cleaned_text)
            return evaluation
            
        except Exception as e:
            logger.error(f"Error parsing evaluation response: {str(e)}")
            return self._get_fallback_evaluation()
    
    def _parse_feedback_response(self, response_text: str) -> Dict[str, Any]:
        """Parse Gemini response for feedback"""
        try:
            cleaned_text = response_text.strip()
            if cleaned_text.startswith('```json'):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith('```'):
                cleaned_text = cleaned_text[:-3]
            
            feedback = json.loads(cleaned_text)
            return feedback
            
        except Exception as e:
            logger.error(f"Error parsing feedback response: {str(e)}")
            return self._get_fallback_feedback()
    
    def _parse_followup_response(self, response_text: str) -> List[str]:
        """Parse Gemini response for follow-up questions"""
        try:
            cleaned_text = response_text.strip()
            if cleaned_text.startswith('```json'):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith('```'):
                cleaned_text = cleaned_text[:-3]
            
            follow_ups = json.loads(cleaned_text)
            return follow_ups if isinstance(follow_ups, list) else []
            
        except Exception as e:
            logger.error(f"Error parsing follow-up response: {str(e)}")
            return []
    
    def _store_questions(
        self, 
        questions_data: List[Dict[str, Any]], 
        role: str, 
        difficulty: str, 
        question_type: str
    ):
        """Store generated questions in database (avoiding duplicates)"""
        try:
            stored_count = 0
            duplicate_count = 0
            
            for q_data in questions_data:
                question_content = q_data['question'].strip()
                
                # Skip empty questions
                if not question_content:
                    continue
                
                # Normalize question content for better duplicate detection
                normalized_content = self._normalize_question_content(question_content)
                
                # Check for exact duplicates (case-insensitive)
                existing_question = self.db.query(Question).filter(
                    Question.content.ilike(question_content)
                ).first()
                
                if existing_question:
                    duplicate_count += 1
                    logger.debug(f"Exact duplicate found, skipping: {question_content[:50]}...")
                    continue
                
                # Check for similar questions using normalized content
                # Look for questions that start with similar text (first 80 characters)
                similar_prefix = normalized_content[:80] if len(normalized_content) > 80 else normalized_content
                similar_question = self.db.query(Question).filter(
                    Question.content.ilike(f"{similar_prefix}%")
                ).first()
                
                if similar_question:
                    # Additional check: calculate similarity ratio
                    similarity_ratio = self._calculate_similarity(
                        normalized_content, 
                        similar_question.content.lower().strip()
                    )
                    
                    # If similarity is above 70%, consider it a duplicate
                    if similarity_ratio > 0.7:
                        duplicate_count += 1
                        logger.debug(f"Similar question found (similarity: {similarity_ratio:.2f}), skipping: {question_content[:50]}...")
                        continue
                
                # Check for questions with same role, type, and very similar content
                role_type_questions = self.db.query(Question).filter(
                    Question.role_category == role,
                    Question.question_type == q_data['category']
                ).all()
                
                is_duplicate = False
                for existing in role_type_questions:
                    existing_normalized = self._normalize_question_content(existing.content)
                    similarity = self._calculate_similarity(normalized_content, existing_normalized)
                    
                    if similarity > 0.75:  # High similarity threshold for same role/type
                        duplicate_count += 1
                        logger.debug(f"Role-specific duplicate found (similarity: {similarity:.2f}), skipping: {question_content[:50]}...")
                        is_duplicate = True
                        break
                
                if is_duplicate:
                    continue
                
                # Store new question
                question = Question(
                    content=question_content,
                    question_type=q_data['category'],
                    role_category=role,
                    difficulty_level=difficulty,
                    expected_duration=q_data['duration'],
                    generated_by='gemini_api'
                )
                self.db.add(question)
                stored_count += 1
                logger.debug(f"Storing new question: {question_content[:50]}...")
            
            if stored_count > 0:
                self.db.commit()
                logger.info(f"Stored {stored_count} new questions in database")
            
            if duplicate_count > 0:
                logger.info(f"Skipped {duplicate_count} duplicate/similar questions")
            
        except Exception as e:
            logger.error(f"Error storing questions: {str(e)}")
            self.db.rollback()
    
    def _normalize_question_content(self, content: str) -> str:
        """Normalize question content for better duplicate detection"""
        import re
        
        # Convert to lowercase
        normalized = content.lower().strip()
        
        # Remove extra whitespace
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # Remove common punctuation that doesn't affect meaning
        normalized = re.sub(r'[.!?;,]', '', normalized)
        
        # Remove common question starters that might vary
        starters = [
            'can you ', 'could you ', 'would you ', 'will you ',
            'please ', 'tell me ', 'describe ', 'explain ',
            'what is ', 'what are ', 'how do ', 'how would ',
            'why do ', 'why would ', 'when do ', 'where do '
        ]
        
        for starter in starters:
            if normalized.startswith(starter):
                normalized = normalized[len(starter):]
                break
        
        return normalized.strip()
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate similarity ratio between two texts using multiple methods"""
        if not text1 or not text2:
            return 0.0
        
        # Method 1: Word-based Jaccard similarity
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
        
        jaccard_similarity = len(words1.intersection(words2)) / len(words1.union(words2))
        
        # Method 2: Character-based similarity (for catching minor variations)
        chars1 = set(text1.replace(' ', ''))
        chars2 = set(text2.replace(' ', ''))
        char_similarity = len(chars1.intersection(chars2)) / len(chars1.union(chars2)) if chars1.union(chars2) else 0
        
        # Method 3: Length-based similarity check
        len1, len2 = len(text1), len(text2)
        length_ratio = min(len1, len2) / max(len1, len2) if max(len1, len2) > 0 else 0
        
        # Method 4: Common substring check for question variations
        shorter, longer = (text1, text2) if len1 < len2 else (text2, text1)
        substring_ratio = 0
        if len(shorter) > 10:
            shorter_words = shorter.split()
            longer_words = longer.split()
            common_words = sum(1 for word in shorter_words if word in longer_words)
            substring_ratio = common_words / len(shorter_words) if shorter_words else 0
        
        # Combine similarities with weights optimized for question detection
        combined_similarity = (
            jaccard_similarity * 0.4 +
            substring_ratio * 0.3 +
            char_similarity * 0.2 +
            length_ratio * 0.1
        )
        
        return combined_similarity
    
    def _get_technical_competencies(self, main_role: str, sub_role: str = None, specialization: str = None) -> List[str]:
        """Map role hierarchy to specific technical competencies that should be assessed"""
        
        competency_map = {
            "Software Developer": {
                "Frontend Developer": {
                    "React Developer": [
                        "React component architecture", "State management (Redux/Context)", 
                        "JavaScript ES6+", "TypeScript", "CSS-in-JS", "Performance optimization",
                        "Testing (Jest/React Testing Library)", "Build tools (Webpack/Vite)"
                    ],
                    "Vue Developer": [
                        "Vue.js composition API", "Vuex state management", "Vue Router",
                        "JavaScript/TypeScript", "Component lifecycle", "Vue CLI/Vite"
                    ],
                    "Angular Developer": [
                        "Angular framework", "TypeScript", "RxJS", "Angular CLI",
                        "Dependency injection", "Angular Material", "NgRx"
                    ]
                },
                "Backend Developer": {
                    "Node.js Developer": [
                        "Node.js runtime", "Express.js", "Database design", "RESTful APIs",
                        "Authentication/Authorization", "Microservices", "Testing frameworks"
                    ],
                    "Python Developer": [
                        "Python frameworks (Django/Flask)", "Database ORM", "API design",
                        "Async programming", "Testing (pytest)", "Package management"
                    ],
                    "Java Developer": [
                        "Spring framework", "JPA/Hibernate", "Maven/Gradle", "JUnit testing",
                        "Microservices", "Database design", "Design patterns"
                    ]
                },
                "Mobile Developer": {
                    "iOS Developer": [
                        "Swift programming", "iOS SDK", "UIKit/SwiftUI", "Core Data",
                        "Xcode", "App Store guidelines", "iOS design patterns"
                    ],
                    "Android Developer": [
                        "Kotlin/Java", "Android SDK", "Jetpack Compose", "Room database",
                        "Android Studio", "Material Design", "Architecture patterns"
                    ]
                },
                "Full Stack Developer": {
                    "MEAN Stack": ["MongoDB", "Express.js", "Angular", "Node.js", "RESTful APIs"],
                    "MERN Stack": ["MongoDB", "Express.js", "React", "Node.js", "State management"]
                }
            },
            "Data Scientist": {
                "ML Engineer": {
                    "Computer Vision Engineer": [
                        "Deep learning frameworks", "Image processing", "CNN architectures",
                        "OpenCV", "Model deployment", "Data preprocessing", "Performance optimization"
                    ],
                    "NLP Engineer": [
                        "Natural language processing", "Transformer models", "Text preprocessing",
                        "BERT/GPT models", "Tokenization", "Language model fine-tuning"
                    ]
                },
                "Data Analyst": {
                    "Business Intelligence Analyst": [
                        "SQL querying", "Data visualization", "Statistical analysis",
                        "Business metrics", "Dashboard creation", "Data warehousing"
                    ]
                },
                "Research Scientist": {
                    "AI Researcher": [
                        "Research methodology", "Statistical modeling", "Algorithm development",
                        "Paper writing", "Experimental design", "Peer review process"
                    ]
                }
            },
            "DevOps Engineer": {
                "Site Reliability Engineer": {
                    "Platform SRE": [
                        "Kubernetes orchestration", "Monitoring/Alerting", "Incident response",
                        "Service level objectives", "Capacity planning", "Automation"
                    ]
                },
                "Cloud Engineer": {
                    "AWS Engineer": [
                        "AWS services", "Infrastructure as Code", "CI/CD pipelines",
                        "Security best practices", "Cost optimization", "Disaster recovery"
                    ]
                }
            },
            "Product Manager": {
                "Technical Product Manager": {
                    "API Product Manager": [
                        "API design principles", "Developer experience", "Technical roadmapping",
                        "Integration strategies", "Performance metrics", "Documentation"
                    ]
                },
                "Growth Product Manager": {
                    "User Acquisition PM": [
                        "Growth metrics", "A/B testing", "User funnel analysis",
                        "Conversion optimization", "Marketing technology", "Analytics"
                    ]
                }
            }
        }
        
        # Get competencies based on role hierarchy
        competencies = []
        
        if main_role in competency_map:
            role_data = competency_map[main_role]
            
            if sub_role and sub_role in role_data:
                sub_role_data = role_data[sub_role]
                
                if specialization and specialization in sub_role_data:
                    competencies = sub_role_data[specialization]
                else:
                    # Get all competencies for the sub_role
                    for spec_competencies in sub_role_data.values():
                        competencies.extend(spec_competencies)
            else:
                # Get all competencies for the main_role
                for sub_data in role_data.values():
                    for spec_competencies in sub_data.values():
                        competencies.extend(spec_competencies)
        
        # Remove duplicates while preserving order
        return list(dict.fromkeys(competencies))
    
    def generate_technical_questions(
        self,
        role_data: Dict[str, Any],
        difficulty: str = "intermediate",
        count: int = 5,
        question_focus: str = "mixed"
    ) -> List[Dict[str, Any]]:
        """Generate technical interview questions specific to role and difficulty"""
        
        try:
            main_role = self._get_role_attribute(role_data, 'main_role', '')
            sub_role = self._get_role_attribute(role_data, 'sub_role', '')
            specialization = self._get_role_attribute(role_data, 'specialization', '')
            tech_stacks = self._get_role_attribute(role_data, 'tech_stacks', [])
            
            logger.info(f"Generating technical questions for {main_role}/{sub_role}/{specialization}")
            
            # For mixed focus, generate a variety of technical question types
            if question_focus == "mixed":
                all_questions = []
                
                # Distribute questions across different categories
                db_count = max(1, count // 4)  # 25% database questions
                algo_count = max(1, count // 3)  # 33% algorithm questions
                system_count = max(1, count // 4)  # 25% system design questions
                debug_count = max(1, count - db_count - algo_count - system_count)  # Remaining for debugging
                
                # Generate database questions
                try:
                    db_questions = self.generate_database_questions(role_data, difficulty, db_count)
                    all_questions.extend(db_questions)
                except Exception as e:
                    logger.warning(f"Failed to generate database questions: {e}")
                
                # Generate algorithm questions
                try:
                    algo_questions = self.generate_algorithm_questions(role_data, difficulty, algo_count)
                    all_questions.extend(algo_questions)
                except Exception as e:
                    logger.warning(f"Failed to generate algorithm questions: {e}")
                
                # Generate system design questions
                try:
                    system_questions = self.generate_system_design_questions(role_data, difficulty, system_count)
                    all_questions.extend(system_questions)
                except Exception as e:
                    logger.warning(f"Failed to generate system design questions: {e}")
                
                # Generate debugging questions
                try:
                    debug_questions = self.generate_debugging_questions(role_data, difficulty, debug_count)
                    all_questions.extend(debug_questions)
                except Exception as e:
                    logger.warning(f"Failed to generate debugging questions: {e}")
                
                # If we have enough questions, return them
                if len(all_questions) >= count:
                    return all_questions[:count]
                
                # If not enough specialized questions, fall back to general technical generation
                logger.warning(f"Only generated {len(all_questions)} specialized questions, falling back to general generation")
            
            # For specific focus or fallback, use the general technical question prompt
            prompt = self._build_technical_question_prompt(
                main_role, sub_role, specialization, tech_stacks, difficulty, count, question_focus
            )
            
            generation_config = {
                "temperature": 0.7,  # Balanced creativity for technical questions
                "top_p": 0.9,
                "top_k": 40,
                "max_output_tokens": 4096,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            questions_data = self._parse_technical_questions_response(response_text)
            
            if questions_data:
                logger.info(f"Generated {len(questions_data)} technical questions")
                return questions_data
            else:
                logger.warning("Failed to generate technical questions, using fallback")
                return self._get_fallback_technical_questions(main_role, sub_role, difficulty, count)
                
        except Exception as e:
            logger.error(f"Error generating technical questions: {str(e)}")
            return self._get_fallback_technical_questions(
                self._get_role_attribute(role_data, 'main_role', ''), 
                self._get_role_attribute(role_data, 'sub_role', ''), 
                difficulty, 
                count
            )
    
    def _build_technical_question_prompt(
        self,
        main_role: str,
        sub_role: str,
        specialization: str,
        tech_stacks: List[str],
        difficulty: str,
        count: int,
        question_focus: str
    ) -> str:
        """Build comprehensive technical question generation prompt"""
        
        # Get role-specific technical templates
        technical_templates = self._get_technical_question_templates(main_role, sub_role, specialization)
        
        difficulty_specs = {
            "beginner": {
                "description": "Entry-level (0-2 years experience)",
                "focus": "Basic concepts, syntax, simple problem-solving",
                "complexity": "Straightforward scenarios with clear solutions"
            },
            "intermediate": {
                "description": "Mid-level (2-5 years experience)", 
                "focus": "Applied knowledge, best practices, moderate complexity",
                "complexity": "Real-world scenarios requiring analysis and trade-offs"
            },
            "advanced": {
                "description": "Senior-level (5+ years experience)",
                "focus": "Architecture decisions, optimization, leadership scenarios",
                "complexity": "Complex systems, performance at scale, strategic thinking"
            }
        }
        
        focus_instructions = {
            "database": "Focus on database design, querying, optimization, and data modeling",
            "algorithms": "Focus on data structures, algorithms, complexity analysis, and problem-solving",
            "system_design": "Focus on architecture, scalability, distributed systems, and design patterns",
            "debugging": "Focus on code review, debugging scenarios, and troubleshooting",
            "mixed": "Include a variety of technical areas relevant to the role"
        }
        
        prompt = f"""
        You are a senior technical interviewer creating challenging, role-specific technical questions.
        
        ROLE CONTEXT:
        - Main Role: {main_role}
        - Sub Role: {sub_role}
        - Specialization: {specialization}
        - Required Technologies: {', '.join(tech_stacks[:5]) if tech_stacks else 'Standard tools'}
        
        DIFFICULTY REQUIREMENTS:
        - Level: {difficulty_specs.get(difficulty, difficulty_specs['intermediate'])['description']}
        - Focus: {difficulty_specs.get(difficulty, difficulty_specs['intermediate'])['focus']}
        - Complexity: {difficulty_specs.get(difficulty, difficulty_specs['intermediate'])['complexity']}
        
        QUESTION FOCUS: {focus_instructions.get(question_focus, focus_instructions['mixed'])}
        
        TECHNICAL QUESTION CATEGORIES TO INCLUDE:
        {technical_templates}
        
        SPECIFIC REQUIREMENTS:
        1. Generate exactly {count} technical questions
        2. Each question must test specific technical knowledge for this role
        3. Include practical, hands-on scenarios the candidate would face
        4. Questions should differentiate between skill levels
        5. Reference specific technologies from the tech stack when relevant
        6. Include both theoretical knowledge and practical application
        
        QUESTION TYPES TO INCLUDE:
        - Database Query Problems: SQL optimization, schema design, performance tuning
        - Algorithm Challenges: Data structure selection, complexity analysis, optimization
        - System Design: Architecture decisions, scalability, trade-offs
        - Code Review: Bug identification, best practices, refactoring
        - Problem Solving: Real-world scenarios requiring technical solutions
        - Technology-Specific: Framework/tool-specific implementation questions
        
        For each question, provide:
        1. Question text (specific technical scenario or problem)
        2. Category (database, algorithm, system-design, debugging, implementation, architecture)
        3. Expected duration (3-8 minutes for technical questions)
        4. Key technical concepts being tested
        5. Sample answer outline (what a good response should cover)
        6. Difficulty justification (why this is appropriate for the level)
        
        Format as JSON:
        [
            {{
                "question": "Detailed technical question with specific scenario",
                "category": "database|algorithm|system-design|debugging|implementation|architecture",
                "duration": 5,
                "key_concepts": ["concept 1", "concept 2", "concept 3"],
                "sample_answer_outline": ["point 1", "point 2", "point 3"],
                "difficulty_justification": "Why this tests {difficulty} level skills"
            }}
        ]
        
        Generate technical questions now:
        """
        
        return prompt
    
    def _get_technical_question_templates(self, main_role: str, sub_role: str, specialization: str) -> str:
        """Get role-specific technical question templates and focus areas"""
        
        templates = {
            "Software Developer": {
                "Frontend Developer": {
                    "React Developer": """
                    - React component lifecycle and hooks optimization
                    - State management patterns (Redux, Context API, Zustand)
                    - Performance optimization (memoization, code splitting, lazy loading)
                    - Testing strategies (unit tests, integration tests, E2E)
                    - Build tools and bundling optimization (Webpack, Vite)
                    - CSS-in-JS and styling architecture
                    - Browser compatibility and polyfills
                    """,
                    "Vue Developer": """
                    - Vue.js composition API and reactivity system
                    - Vuex/Pinia state management patterns
                    - Vue Router and navigation guards
                    - Component communication and event handling
                    - Vue CLI and build optimization
                    - Testing with Vue Test Utils
                    - Performance optimization techniques
                    """,
                    "Angular Developer": """
                    - Angular framework architecture and dependency injection
                    - RxJS observables and reactive programming
                    - Angular CLI and workspace configuration
                    - Component lifecycle and change detection
                    - NgRx state management
                    - Angular Material and UI components
                    - Testing with Jasmine and Karma
                    """
                },
                "Backend Developer": {
                    "Node.js Developer": """
                    - Node.js event loop and asynchronous programming
                    - Express.js middleware and routing
                    - Database integration (MongoDB, PostgreSQL, Redis)
                    - RESTful API design and GraphQL
                    - Authentication and authorization (JWT, OAuth)
                    - Microservices architecture and communication
                    - Testing frameworks (Jest, Mocha, Supertest)
                    - Performance monitoring and optimization
                    """,
                    "Python Developer": """
                    - Python frameworks (Django, Flask, FastAPI)
                    - Database ORM (SQLAlchemy, Django ORM)
                    - Async programming (asyncio, async/await)
                    - API design and documentation
                    - Testing (pytest, unittest, mocking)
                    - Package management and virtual environments
                    - Performance optimization and profiling
                    """,
                    "Java Developer": """
                    - Spring framework (Boot, Security, Data)
                    - JPA/Hibernate and database optimization
                    - Maven/Gradle build tools
                    - JUnit and Mockito testing
                    - Microservices with Spring Cloud
                    - Design patterns and SOLID principles
                    - JVM performance tuning
                    """
                },
                "Mobile Developer": {
                    "iOS Developer": """
                    - Swift programming and iOS SDK
                    - UIKit and SwiftUI framework differences
                    - Core Data and data persistence
                    - Networking and API integration
                    - App Store guidelines and submission process
                    - iOS design patterns (MVC, MVVM, VIPER)
                    - Performance optimization and memory management
                    """,
                    "Android Developer": """
                    - Kotlin/Java and Android SDK
                    - Jetpack Compose vs traditional Views
                    - Room database and data persistence
                    - Architecture patterns (MVVM, MVP, Clean Architecture)
                    - Material Design implementation
                    - Play Store optimization and guidelines
                    - Performance profiling and optimization
                    """
                },
                "Full Stack Developer": {
                    "MEAN Stack": """
                    - MongoDB database design and optimization
                    - Express.js server architecture
                    - Angular frontend development
                    - Node.js backend development
                    - RESTful API design and implementation
                    - Authentication and session management
                    - Deployment and DevOps practices
                    """,
                    "MERN Stack": """
                    - MongoDB database operations
                    - Express.js middleware and routing
                    - React component architecture
                    - Node.js server development
                    - State management across the stack
                    - Real-time features with Socket.io
                    - Testing full-stack applications
                    """
                }
            },
            "Data Scientist": {
                "ML Engineer": {
                    "Computer Vision Engineer": """
                    - Deep learning frameworks (TensorFlow, PyTorch)
                    - CNN architectures and transfer learning
                    - Image preprocessing and augmentation
                    - Object detection and segmentation
                    - Model deployment and optimization
                    - OpenCV and image processing
                    - Performance optimization for inference
                    """,
                    "NLP Engineer": """
                    - Natural language processing pipelines
                    - Transformer models (BERT, GPT, T5)
                    - Text preprocessing and tokenization
                    - Named entity recognition and sentiment analysis
                    - Language model fine-tuning
                    - Hugging Face transformers library
                    - Text generation and summarization
                    """
                },
                "Data Analyst": {
                    "Business Intelligence Analyst": """
                    - SQL query optimization and complex joins
                    - Data visualization tools (Tableau, Power BI)
                    - Statistical analysis and hypothesis testing
                    - Business metrics and KPI development
                    - Dashboard design and storytelling
                    - Data warehousing concepts
                    - ETL processes and data pipelines
                    """
                },
                "Research Scientist": {
                    "AI Researcher": """
                    - Research methodology and experimental design
                    - Statistical modeling and hypothesis testing
                    - Algorithm development and optimization
                    - Paper writing and peer review process
                    - Literature review and state-of-the-art analysis
                    - Reproducible research practices
                    - Conference presentation and publication
                    """
                }
            },
            "DevOps Engineer": {
                "Site Reliability Engineer": {
                    "Platform SRE": """
                    - Kubernetes orchestration and management
                    - Monitoring and alerting systems (Prometheus, Grafana)
                    - Incident response and post-mortem analysis
                    - Service level objectives and error budgets
                    - Capacity planning and performance optimization
                    - Infrastructure automation and IaC
                    - Disaster recovery and backup strategies
                    """
                },
                "Cloud Engineer": {
                    "AWS Engineer": """
                    - AWS services architecture and best practices
                    - Infrastructure as Code (Terraform, CloudFormation)
                    - CI/CD pipeline design and implementation
                    - Security best practices and compliance
                    - Cost optimization and resource management
                    - Serverless architecture and Lambda functions
                    - Container orchestration with ECS/EKS
                    """
                }
            },
            "Product Manager": {
                "Technical Product Manager": {
                    "API Product Manager": """
                    - API design principles and best practices
                    - Developer experience and documentation
                    - Technical roadmapping and prioritization
                    - Integration strategies and partnerships
                    - API performance metrics and analytics
                    - Versioning and backward compatibility
                    - Developer community building
                    """
                },
                "Growth Product Manager": {
                    "User Acquisition PM": """
                    - Growth metrics and funnel analysis
                    - A/B testing and experimentation
                    - User acquisition channels and optimization
                    - Conversion rate optimization
                    - Marketing technology stack
                    - Analytics and attribution modeling
                    - Retention and engagement strategies
                    """
                }
            }
        }
        
        # Get role-specific templates
        role_templates = ""
        if main_role in templates:
            role_data = templates[main_role]
            if sub_role and sub_role in role_data:
                sub_role_data = role_data[sub_role]
                if specialization and specialization in sub_role_data:
                    role_templates = sub_role_data[specialization]
                else:
                    # Combine all specializations for the sub_role
                    all_templates = []
                    for spec_template in sub_role_data.values():
                        all_templates.append(spec_template)
                    role_templates = "\n".join(all_templates)
            else:
                # Combine all sub_roles for the main_role
                all_templates = []
                for sub_data in role_data.values():
                    for spec_template in sub_data.values():
                        all_templates.append(spec_template)
                role_templates = "\n".join(all_templates)
        
        return role_templates or f"General {main_role} technical competencies and best practices"
    def _parse_technical_questions_response(self, response_text: str) -> List[Dict[str, Any]]:
        """Parse technical questions response with enhanced validation"""
        
        try:
            # Clean the response text
            cleaned_text = response_text.strip()
            if cleaned_text.startswith('```json'):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith('```'):
                cleaned_text = cleaned_text[:-3]
            
            questions = json.loads(cleaned_text)
            
            # Validate and enhance technical questions
            validated_questions = []
            for q in questions:
                if isinstance(q, dict) and 'question' in q:
                    validated_question = {
                        'question': q.get('question', ''),
                        'category': q.get('category', 'technical'),
                        'duration': q.get('duration', 5),
                        'key_points': q.get('key_concepts', q.get('key_points', [])),
                        'sample_answer_outline': q.get('sample_answer_outline', []),
                        'difficulty_justification': q.get('difficulty_justification', ''),
                        'question_type': 'technical'  # Mark as technical question
                    }
                    
                    # Ensure minimum quality standards
                    if (len(validated_question['question']) > 20 and 
                        validated_question['category'] and
                        validated_question['key_points']):
                        validated_questions.append(validated_question)
            
            return validated_questions
            
        except Exception as e:
            logger.error(f"Error parsing technical questions response: {str(e)}")
            return []
    
    def _get_fallback_technical_questions(
        self, 
        main_role: str, 
        sub_role: str, 
        difficulty: str, 
        count: int
    ) -> List[Dict[str, Any]]:
        """Generate fallback technical questions when AI generation fails"""
        
        # Role-specific technical question banks
        technical_questions = {
            "Software Developer": {
                "beginner": [
                    {
                        "question": "Explain the difference between == and === in JavaScript. When would you use each?",
                        "category": "technical",
                        "duration": 3,
                        "key_points": ["Type coercion", "Strict equality", "Best practices"],
                        "sample_answer_outline": ["== performs type coercion", "=== checks type and value", "Use === for safer comparisons"]
                    },
                    {
                        "question": "What is the purpose of version control systems like Git? Describe a basic Git workflow.",
                        "category": "technical", 
                        "duration": 4,
                        "key_points": ["Version control benefits", "Git commands", "Branching strategy"],
                        "sample_answer_outline": ["Track changes", "Collaboration", "Basic commands (add, commit, push)"]
                    }
                ],
                "intermediate": [
                    {
                        "question": "Design a RESTful API for a blog system. What endpoints would you create and what HTTP methods would you use?",
                        "category": "system-design",
                        "duration": 6,
                        "key_points": ["REST principles", "HTTP methods", "Resource design"],
                        "sample_answer_outline": ["GET /posts", "POST /posts", "PUT /posts/:id", "DELETE /posts/:id"]
                    },
                    {
                        "question": "Explain database indexing. How would you optimize a slow query that searches for users by email?",
                        "category": "database",
                        "duration": 5,
                        "key_points": ["Index types", "Query optimization", "Performance impact"],
                        "sample_answer_outline": ["Create index on email column", "Explain query execution plan", "Consider unique constraints"]
                    }
                ],
                "advanced": [
                    {
                        "question": "Design a distributed caching system for a high-traffic e-commerce site. How would you handle cache invalidation?",
                        "category": "system-design",
                        "duration": 8,
                        "key_points": ["Distributed systems", "Cache strategies", "Consistency"],
                        "sample_answer_outline": ["Redis cluster", "Cache-aside pattern", "TTL and invalidation strategies"]
                    }
                ]
            },
            "Data Scientist": {
                "beginner": [
                    {
                        "question": "Explain the difference between supervised and unsupervised learning. Give examples of each.",
                        "category": "technical",
                        "duration": 4,
                        "key_points": ["Learning types", "Examples", "Use cases"],
                        "sample_answer_outline": ["Supervised uses labeled data", "Unsupervised finds patterns", "Classification vs clustering examples"]
                    }
                ],
                "intermediate": [
                    {
                        "question": "How would you handle missing data in a dataset? Describe different imputation strategies.",
                        "category": "technical",
                        "duration": 5,
                        "key_points": ["Missing data types", "Imputation methods", "Impact on analysis"],
                        "sample_answer_outline": ["Mean/median imputation", "Forward fill", "Model-based imputation"]
                    }
                ],
                "advanced": [
                    {
                        "question": "Design an A/B testing framework for a recommendation system. How would you measure statistical significance?",
                        "category": "system-design",
                        "duration": 8,
                        "key_points": ["Experimental design", "Statistical testing", "Metrics"],
                        "sample_answer_outline": ["Random assignment", "Power analysis", "Multiple testing correction"]
                    }
                ]
            },
            "DevOps Engineer": {
                "beginner": [
                    {
                        "question": "Explain the difference between containers and virtual machines. When would you use each?",
                        "category": "technical",
                        "duration": 4,
                        "key_points": ["Containerization", "Virtualization", "Use cases"],
                        "sample_answer_outline": ["Resource usage", "Isolation levels", "Deployment scenarios"]
                    }
                ],
                "intermediate": [
                    {
                        "question": "Design a CI/CD pipeline for a web application. What stages would you include?",
                        "category": "system-design",
                        "duration": 6,
                        "key_points": ["Pipeline stages", "Testing", "Deployment"],
                        "sample_answer_outline": ["Build", "Test", "Security scan", "Deploy", "Monitor"]
                    }
                ],
                "advanced": [
                    {
                        "question": "How would you design a monitoring and alerting system for a microservices architecture?",
                        "category": "system-design",
                        "duration": 8,
                        "key_points": ["Observability", "Metrics", "Distributed tracing"],
                        "sample_answer_outline": ["Prometheus/Grafana", "Log aggregation", "Service mesh monitoring"]
                    }
                ]
            }
        }
        
        # Get questions for the role and difficulty
        role_questions = technical_questions.get(main_role, technical_questions["Software Developer"])
        difficulty_questions = role_questions.get(difficulty, role_questions.get("intermediate", []))
        
        # Return requested number of questions (cycle through if needed)
        result = []
        for i in range(count):
            if difficulty_questions:
                question = difficulty_questions[i % len(difficulty_questions)].copy()
                result.append(question)
        
        return result
    
    def generate_database_questions(
        self,
        role_data: Dict[str, Any],
        difficulty: str = "intermediate",
        count: int = 3,
        db_type: str = "mixed"
    ) -> List[Dict[str, Any]]:
        """Generate database-specific questions (SQL, NoSQL optimization problems)"""
        
        try:
            logger.info(f"Generating database questions for {self._get_role_attribute(role_data, 'main_role', 'Unknown')}")
            
            prompt = self._build_database_question_prompt(role_data, difficulty, count, db_type)
            
            generation_config = {
                "temperature": 0.6,
                "top_p": 0.9,
                "max_output_tokens": 3072,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            questions_data = self._parse_technical_questions_response(response_text)
            
            if questions_data:
                logger.info(f"Generated {len(questions_data)} database questions")
                return questions_data
            else:
                return self._get_fallback_database_questions(difficulty, count, db_type)
                
        except Exception as e:
            logger.error(f"Error generating database questions: {str(e)}")
            return self._get_fallback_database_questions(difficulty, count, db_type)
    
    def _build_database_question_prompt(
        self,
        role_data: Dict[str, Any],
        difficulty: str,
        count: int,
        db_type: str
    ) -> str:
        """Build database-specific question generation prompt"""
        
        main_role = self._get_role_attribute(role_data, 'main_role', '')
        tech_stacks = self._get_role_attribute(role_data, 'tech_stacks', [])
        
        db_focus = {
            "sql": "Focus on SQL databases (MySQL, PostgreSQL, SQL Server)",
            "nosql": "Focus on NoSQL databases (MongoDB, Redis, Cassandra)",
            "mixed": "Include both SQL and NoSQL database scenarios"
        }
        
        prompt = f"""
        You are a database expert creating technical interview questions for a {main_role} position.
        
        REQUIREMENTS:
        - Generate {count} database-focused questions
        - Difficulty: {difficulty}
        - Database focus: {db_focus.get(db_type, db_focus['mixed'])}
        - Technologies: {', '.join(tech_stacks) if tech_stacks else 'Standard database technologies'}
        
        QUESTION CATEGORIES TO INCLUDE:
        1. SQL Query Optimization
           - Complex joins and subqueries
           - Index usage and performance tuning
           - Query execution plan analysis
           
        2. Database Design
           - Schema design and normalization
           - Relationship modeling
           - Constraint implementation
           
        3. Performance Optimization
           - Slow query identification and fixing
           - Index strategy and maintenance
           - Database scaling approaches
           
        4. NoSQL Scenarios (if applicable)
           - Document structure design
           - Aggregation pipelines
           - Consistency and partitioning
           
        5. Real-world Problems
           - Data migration strategies
           - Backup and recovery
           - Concurrent access handling
        
        Each question should:
        - Present a realistic database scenario
        - Test practical problem-solving skills
        - Include specific technical requirements
        - Be appropriate for the {difficulty} level
        
        Format as JSON:
        [
            {{
                "question": "Detailed database scenario or problem",
                "category": "database",
                "duration": 5,
                "key_concepts": ["concept1", "concept2", "concept3"],
                "sample_answer_outline": ["approach1", "approach2", "approach3"],
                "difficulty_justification": "Why this tests {difficulty} database skills"
            }}
        ]
        
        Generate database questions now:
        """
        
        return prompt
    
    def generate_algorithm_questions(
        self,
        role_data: Dict[str, Any],
        difficulty: str = "intermediate",
        count: int = 3,
        focus_area: str = "mixed"
    ) -> List[Dict[str, Any]]:
        """Generate algorithm and data structure questions"""
        
        try:
            logger.info(f"Generating algorithm questions for {self._get_role_attribute(role_data, 'main_role', 'Unknown')}")
            
            prompt = self._build_algorithm_question_prompt(role_data, difficulty, count, focus_area)
            
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.9,
                "max_output_tokens": 3072,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            questions_data = self._parse_technical_questions_response(response_text)
            
            if questions_data:
                logger.info(f"Generated {len(questions_data)} algorithm questions")
                return questions_data
            else:
                return self._get_fallback_algorithm_questions(difficulty, count, focus_area)
                
        except Exception as e:
            logger.error(f"Error generating algorithm questions: {str(e)}")
            return self._get_fallback_algorithm_questions(difficulty, count, focus_area)
    
    def _build_algorithm_question_prompt(
        self,
        role_data: Dict[str, Any],
        difficulty: str,
        count: int,
        focus_area: str
    ) -> str:
        """Build algorithm-specific question generation prompt"""
        
        main_role = self._get_role_attribute(role_data, 'main_role', '')
        tech_stacks = self._get_role_attribute(role_data, 'tech_stacks', [])
        
        focus_instructions = {
            "arrays": "Focus on array manipulation, searching, and sorting algorithms",
            "trees": "Focus on tree traversal, binary search trees, and tree algorithms",
            "graphs": "Focus on graph algorithms, shortest path, and network problems",
            "dynamic_programming": "Focus on optimization problems and dynamic programming",
            "mixed": "Include a variety of algorithmic problem types"
        }
        
        prompt = f"""
        You are an algorithms expert creating coding interview questions for a {main_role} position.
        
        REQUIREMENTS:
        - Generate {count} algorithm and data structure questions
        - Difficulty: {difficulty}
        - Focus area: {focus_instructions.get(focus_area, focus_instructions['mixed'])}
        - Consider technologies: {', '.join(tech_stacks) if tech_stacks else 'Language-agnostic'}
        
        ALGORITHM CATEGORIES TO INCLUDE:
        1. Data Structure Selection
           - When to use arrays vs linked lists vs hash tables
           - Tree vs graph representations
           - Stack vs queue applications
           
        2. Algorithm Design
           - Sorting and searching algorithms
           - Recursion and iteration trade-offs
           - Time and space complexity analysis
           
        3. Problem Solving Patterns
           - Two pointers technique
           - Sliding window problems
           - Divide and conquer approaches
           
        4. Optimization Problems
           - Dynamic programming scenarios
           - Greedy algorithm applications
           - Backtracking problems
           
        5. Real-world Applications
           - System design algorithm choices
           - Performance optimization scenarios
           - Scalability considerations
        
        Each question should:
        - Present a clear problem statement
        - Test algorithmic thinking
        - Include complexity analysis requirements
        - Be solvable within interview timeframe
        
        Format as JSON:
        [
            {{
                "question": "Clear algorithm problem with constraints and examples",
                "category": "algorithm",
                "duration": 6,
                "key_concepts": ["data_structure", "algorithm_type", "complexity"],
                "sample_answer_outline": ["approach", "implementation_notes", "complexity_analysis"],
                "difficulty_justification": "Why this tests {difficulty} algorithmic skills"
            }}
        ]
        
        Generate algorithm questions now:
        """
        
        return prompt
    
    def generate_system_design_questions(
        self,
        role_data: Dict[str, Any],
        difficulty: str = "intermediate",
        count: int = 2,
        system_type: str = "web_application"
    ) -> List[Dict[str, Any]]:
        """Generate system design and architecture questions"""
        
        try:
            logger.info(f"Generating system design questions for {self._get_role_attribute(role_data, 'main_role', 'Unknown')}")
            
            prompt = self._build_system_design_prompt(role_data, difficulty, count, system_type)
            
            generation_config = {
                "temperature": 0.8,
                "top_p": 0.9,
                "max_output_tokens": 3072,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            questions_data = self._parse_technical_questions_response(response_text)
            
            if questions_data:
                logger.info(f"Generated {len(questions_data)} system design questions")
                return questions_data
            else:
                return self._get_fallback_system_design_questions(difficulty, count, system_type)
                
        except Exception as e:
            logger.error(f"Error generating system design questions: {str(e)}")
            return self._get_fallback_system_design_questions(difficulty, count, system_type)
    
    def _build_system_design_prompt(
        self,
        role_data: Dict[str, Any],
        difficulty: str,
        count: int,
        system_type: str
    ) -> str:
        """Build system design question generation prompt"""
        
        main_role = self._get_role_attribute(role_data, 'main_role', '')
        tech_stacks = self._get_role_attribute(role_data, 'tech_stacks', [])
        
        system_types = {
            "web_application": "Web applications and services",
            "distributed_system": "Distributed systems and microservices",
            "data_pipeline": "Data processing and analytics systems",
            "mobile_backend": "Mobile application backend systems"
        }
        
        prompt = f"""
        You are a system architecture expert creating design interview questions for a {main_role} position.
        
        REQUIREMENTS:
        - Generate {count} system design questions
        - Difficulty: {difficulty}
        - System focus: {system_types.get(system_type, system_types['web_application'])}
        - Technologies: {', '.join(tech_stacks) if tech_stacks else 'Modern tech stack'}
        
        SYSTEM DESIGN AREAS TO COVER:
        1. Architecture Patterns
           - Microservices vs monolith decisions
           - Event-driven architecture
           - API gateway patterns
           
        2. Scalability and Performance
           - Load balancing strategies
           - Caching layers and strategies
           - Database scaling approaches
           
        3. Reliability and Availability
           - Fault tolerance design
           - Disaster recovery planning
           - Monitoring and alerting
           
        4. Security and Compliance
           - Authentication and authorization
           - Data encryption and privacy
           - Security best practices
           
        5. Technology Selection
           - Database choice justification
           - Framework and tool selection
           - Infrastructure decisions
        
        Each question should:
        - Present a realistic system design challenge
        - Require architectural decision-making
        - Include scalability and performance considerations
        - Test understanding of trade-offs
        
        Format as JSON:
        [
            {{
                "question": "System design challenge with requirements and constraints",
                "category": "system-design",
                "duration": 8,
                "key_concepts": ["architecture", "scalability", "technology_choice"],
                "sample_answer_outline": ["high_level_design", "detailed_components", "trade_offs"],
                "difficulty_justification": "Why this tests {difficulty} system design skills"
            }}
        ]
        
        Generate system design questions now:
        """
        
        return prompt
    
    def generate_code_debugging_questions(
        self,
        role_data: Dict[str, Any],
        difficulty: str = "intermediate",
        count: int = 3,
        language: str = "javascript"
    ) -> List[Dict[str, Any]]:
        """Generate code debugging and review scenario questions"""
        
        try:
            logger.info(f"Generating debugging questions for {self._get_role_attribute(role_data, 'main_role', 'Unknown')}")
            
            prompt = self._build_debugging_question_prompt(role_data, difficulty, count, language)
            
            generation_config = {
                "temperature": 0.6,
                "top_p": 0.9,
                "max_output_tokens": 4096,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            questions_data = self._parse_technical_questions_response(response_text)
            
            if questions_data:
                logger.info(f"Generated {len(questions_data)} debugging questions")
                return questions_data
            else:
                return self._get_fallback_debugging_questions(difficulty, count, language)
                
        except Exception as e:
            logger.error(f"Error generating debugging questions: {str(e)}")
            return self._get_fallback_debugging_questions(difficulty, count, language)
    
    def _build_debugging_question_prompt(
        self,
        role_data: Dict[str, Any],
        difficulty: str,
        count: int,
        language: str
    ) -> str:
        """Build code debugging question generation prompt"""
        
        main_role = self._get_role_attribute(role_data, 'main_role', '')
        tech_stacks = self._get_role_attribute(role_data, 'tech_stacks', [])
        
        prompt = f"""
        You are a senior developer creating code review and debugging questions for a {main_role} position.
        
        REQUIREMENTS:
        - Generate {count} code debugging/review questions
        - Difficulty: {difficulty}
        - Primary language: {language}
        - Technologies: {', '.join(tech_stacks) if tech_stacks else 'Standard development tools'}
        
        DEBUGGING QUESTION TYPES:
        1. Bug Identification
           - Logic errors in code snippets
           - Performance bottlenecks
           - Security vulnerabilities
           
        2. Code Review Scenarios
           - Best practice violations
           - Code quality improvements
           - Refactoring opportunities
           
        3. Troubleshooting Problems
           - Runtime error diagnosis
           - Memory leak identification
           - Concurrency issues
           
        4. Testing and Quality
           - Missing test cases
           - Edge case handling
           - Error handling improvements
        
        Each question should:
        - Include a realistic code snippet with issues
        - Test debugging and analytical skills
        - Require explanation of the problem and solution
        - Be relevant to day-to-day development work
        
        Format as JSON:
        [
            {{
                "question": "Code review/debugging scenario with code snippet",
                "category": "debugging",
                "duration": 5,
                "key_concepts": ["bug_type", "debugging_technique", "best_practice"],
                "sample_answer_outline": ["issue_identification", "root_cause", "solution_approach"],
                "difficulty_justification": "Why this tests {difficulty} debugging skills"
            }}
        ]
        
        Generate debugging questions now:
        """
        
        return prompt
    
    def _get_fallback_database_questions(self, difficulty: str, count: int, db_type: str) -> List[Dict[str, Any]]:
        """Fallback database questions when AI generation fails"""
        
        database_questions = {
            "beginner": [
                {
                    "question": "Write a SQL query to find all users who have placed orders in the last 30 days. Assume you have 'users' and 'orders' tables.",
                    "category": "database",
                    "duration": 4,
                    "key_concepts": ["SQL joins", "Date functions", "Query structure"],
                    "sample_answer_outline": ["JOIN users and orders", "Use DATE functions", "Filter by date range"]
                },
                {
                    "question": "Explain the difference between INNER JOIN and LEFT JOIN. When would you use each?",
                    "category": "database",
                    "duration": 3,
                    "key_concepts": ["JOIN types", "Data relationships", "Query results"],
                    "sample_answer_outline": ["INNER JOIN returns matches only", "LEFT JOIN includes all left table rows", "Use cases for each"]
                }
            ],
            "intermediate": [
                {
                    "question": "You have a slow query that searches products by category and price range. The query takes 5 seconds on a table with 1M records. How would you optimize it?",
                    "category": "database",
                    "duration": 6,
                    "key_concepts": ["Query optimization", "Indexing strategy", "Performance analysis"],
                    "sample_answer_outline": ["Analyze execution plan", "Create composite index", "Consider query rewriting"]
                },
                {
                    "question": "Design a database schema for a social media platform. Include users, posts, comments, and likes. Consider scalability.",
                    "category": "database",
                    "duration": 8,
                    "key_concepts": ["Schema design", "Relationships", "Scalability"],
                    "sample_answer_outline": ["Entity relationships", "Indexing strategy", "Partitioning considerations"]
                }
            ],
            "advanced": [
                {
                    "question": "Design a sharding strategy for a global e-commerce database with 100M+ users. How would you handle cross-shard queries?",
                    "category": "database",
                    "duration": 10,
                    "key_concepts": ["Database sharding", "Distributed systems", "Query routing"],
                    "sample_answer_outline": ["Sharding key selection", "Cross-shard query handling", "Consistency considerations"]
                }
            ]
        }
        
        questions = database_questions.get(difficulty, database_questions["intermediate"])
        return questions[:count] if len(questions) >= count else questions * ((count // len(questions)) + 1)[:count]
    
    def _get_fallback_algorithm_questions(self, difficulty: str, count: int, focus_area: str) -> List[Dict[str, Any]]:
        """Fallback algorithm questions when AI generation fails"""
        
        algorithm_questions = {
            "beginner": [
                {
                    "question": "Given an array of integers, write a function to find the two numbers that add up to a specific target. Return their indices.",
                    "category": "algorithm",
                    "duration": 5,
                    "key_concepts": ["Array traversal", "Hash table", "Two-pointer technique"],
                    "sample_answer_outline": ["Brute force O(n²)", "Hash table O(n)", "Two-pointer for sorted array"]
                },
                {
                    "question": "Implement a function to reverse a string. What's the time and space complexity?",
                    "category": "algorithm",
                    "duration": 4,
                    "key_concepts": ["String manipulation", "Complexity analysis", "In-place operations"],
                    "sample_answer_outline": ["Character swapping", "Two-pointer approach", "O(n) time, O(1) space"]
                }
            ],
            "intermediate": [
                {
                    "question": "Design an algorithm to find the longest palindromic substring in a given string. Optimize for time complexity.",
                    "category": "algorithm",
                    "duration": 7,
                    "key_concepts": ["Dynamic programming", "String algorithms", "Optimization"],
                    "sample_answer_outline": ["Expand around centers", "Manacher's algorithm", "DP approach comparison"]
                },
                {
                    "question": "Given a binary tree, write a function to determine if it's a valid binary search tree.",
                    "category": "algorithm",
                    "duration": 6,
                    "key_concepts": ["Tree traversal", "BST properties", "Recursion"],
                    "sample_answer_outline": ["In-order traversal", "Min-max bounds", "Recursive validation"]
                }
            ],
            "advanced": [
                {
                    "question": "Design an algorithm to find the shortest path in a weighted graph with negative edges. Handle negative cycles.",
                    "category": "algorithm",
                    "duration": 10,
                    "key_concepts": ["Graph algorithms", "Bellman-Ford", "Negative cycles"],
                    "sample_answer_outline": ["Bellman-Ford algorithm", "Negative cycle detection", "Path reconstruction"]
                }
            ]
        }
        
        questions = algorithm_questions.get(difficulty, algorithm_questions["intermediate"])
        return questions[:count] if len(questions) >= count else questions * ((count // len(questions)) + 1)[:count]
    
    def _get_fallback_system_design_questions(self, difficulty: str, count: int, system_type: str) -> List[Dict[str, Any]]:
        """Fallback system design questions when AI generation fails"""
        
        system_design_questions = {
            "beginner": [
                {
                    "question": "Design a simple URL shortener service like bit.ly. What components would you need?",
                    "category": "system-design",
                    "duration": 8,
                    "key_concepts": ["System components", "Database design", "API design"],
                    "sample_answer_outline": ["URL encoding/decoding", "Database schema", "Caching strategy"]
                }
            ],
            "intermediate": [
                {
                    "question": "Design a chat application that supports real-time messaging for 1M concurrent users. Consider scalability and reliability.",
                    "category": "system-design",
                    "duration": 10,
                    "key_concepts": ["Real-time communication", "Scalability", "Message delivery"],
                    "sample_answer_outline": ["WebSocket connections", "Message queues", "Database partitioning"]
                }
            ],
            "advanced": [
                {
                    "question": "Design a distributed cache system like Redis Cluster. Handle node failures, data consistency, and performance.",
                    "category": "system-design",
                    "duration": 12,
                    "key_concepts": ["Distributed systems", "Consistency", "Fault tolerance"],
                    "sample_answer_outline": ["Consistent hashing", "Replication strategy", "Failure detection"]
                }
            ]
        }
        
        questions = system_design_questions.get(difficulty, system_design_questions["intermediate"])
        return questions[:count] if len(questions) >= count else questions * ((count // len(questions)) + 1)[:count]
    
    def _parse_technical_questions_response(self, response_text: str) -> List[Dict[str, Any]]:
        """Parse Gemini response for technical questions"""
        try:
            # Clean the response text
            cleaned_text = response_text.strip()
            if cleaned_text.startswith('```json'):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith('```'):
                cleaned_text = cleaned_text[:-3]
            
            questions = json.loads(cleaned_text)
            
            # Validate and clean technical questions
            validated_questions = []
            for q in questions:
                if isinstance(q, dict) and 'question' in q:
                    validated_questions.append({
                        'question': q.get('question', ''),
                        'category': q.get('category', 'technical'),
                        'duration': q.get('duration', 5),
                        'key_concepts': q.get('key_concepts', []),
                        'sample_answer_outline': q.get('sample_answer_outline', []),
                        'difficulty_justification': q.get('difficulty_justification', '')
                    })
            
            return validated_questions
            
        except Exception as e:
            logger.error(f"Error parsing technical questions response: {str(e)}")
            return []
    
    def _get_fallback_technical_questions(self, main_role: str, sub_role: str, difficulty: str, count: int) -> List[Dict[str, Any]]:
        """Fallback technical questions when AI generation fails"""
        
        # Role-specific technical question banks
        technical_question_banks = {
            "Software Developer": {
                "beginner": [
                    {
                        "question": "Explain the difference between GET and POST HTTP methods. When would you use each?",
                        "category": "technical",
                        "duration": 4,
                        "key_concepts": ["HTTP methods", "RESTful APIs", "Web protocols"],
                        "sample_answer_outline": ["GET for data retrieval", "POST for data submission", "Idempotency differences"],
                        "difficulty_justification": "Tests basic web development knowledge"
                    },
                    {
                        "question": "What is the difference between == and === in JavaScript? Provide examples.",
                        "category": "technical",
                        "duration": 3,
                        "key_concepts": ["JavaScript operators", "Type coercion", "Strict equality"],
                        "sample_answer_outline": ["Type coercion vs strict comparison", "Examples with different types", "Best practices"],
                        "difficulty_justification": "Fundamental JavaScript concept for beginners"
                    },
                    {
                        "question": "Explain what a database index is and why it's important for query performance.",
                        "category": "database",
                        "duration": 5,
                        "key_concepts": ["Database indexing", "Query optimization", "Performance"],
                        "sample_answer_outline": ["Index structure", "Performance benefits", "Trade-offs"],
                        "difficulty_justification": "Basic database optimization concept"
                    }
                ],
                "intermediate": [
                    {
                        "question": "Design a RESTful API for a blog system with posts, comments, and users. Include the main endpoints and HTTP methods.",
                        "category": "system-design",
                        "duration": 8,
                        "key_concepts": ["REST API design", "Resource modeling", "HTTP methods"],
                        "sample_answer_outline": ["Resource identification", "Endpoint structure", "Status codes"],
                        "difficulty_justification": "Tests API design skills and REST principles"
                    },
                    {
                        "question": "You have a React component that re-renders too frequently, causing performance issues. How would you diagnose and fix this?",
                        "category": "debugging",
                        "duration": 6,
                        "key_concepts": ["React performance", "Re-rendering optimization", "Profiling"],
                        "sample_answer_outline": ["React DevTools profiler", "useMemo/useCallback", "Component optimization"],
                        "difficulty_justification": "Real-world React performance optimization"
                    },
                    {
                        "question": "Implement a function that finds the second largest number in an array. Optimize for time complexity.",
                        "category": "algorithm",
                        "duration": 7,
                        "key_concepts": ["Array algorithms", "Time complexity", "Edge cases"],
                        "sample_answer_outline": ["Single pass solution", "Edge case handling", "O(n) complexity"],
                        "difficulty_justification": "Tests algorithmic thinking and optimization"
                    }
                ],
                "advanced": [
                    {
                        "question": "Design a distributed caching system that can handle 1M requests per second with 99.9% availability. Consider consistency, partitioning, and failure handling.",
                        "category": "system-design",
                        "duration": 12,
                        "key_concepts": ["Distributed systems", "Caching strategies", "High availability"],
                        "sample_answer_outline": ["Consistent hashing", "Replication strategy", "Failure detection"],
                        "difficulty_justification": "Complex distributed systems design"
                    },
                    {
                        "question": "You're leading a team migrating a monolithic application to microservices. What are the key challenges and how would you address them?",
                        "category": "architecture",
                        "duration": 10,
                        "key_concepts": ["Microservices migration", "System architecture", "Team leadership"],
                        "sample_answer_outline": ["Migration strategy", "Service boundaries", "Data consistency"],
                        "difficulty_justification": "Senior-level architectural and leadership skills"
                    }
                ]
            },
            "Data Scientist": {
                "beginner": [
                    {
                        "question": "Explain the difference between supervised and unsupervised learning. Give examples of each.",
                        "category": "technical",
                        "duration": 4,
                        "key_concepts": ["Machine learning types", "Supervised learning", "Unsupervised learning"],
                        "sample_answer_outline": ["Labeled vs unlabeled data", "Classification/regression examples", "Clustering examples"],
                        "difficulty_justification": "Fundamental ML concept for beginners"
                    },
                    {
                        "question": "How would you handle missing values in a dataset? Describe different approaches and when to use them.",
                        "category": "technical",
                        "duration": 5,
                        "key_concepts": ["Data preprocessing", "Missing data", "Imputation methods"],
                        "sample_answer_outline": ["Deletion methods", "Imputation techniques", "Impact on analysis"],
                        "difficulty_justification": "Basic data preprocessing skill"
                    }
                ],
                "intermediate": [
                    {
                        "question": "You're building a recommendation system for an e-commerce platform. Compare collaborative filtering vs content-based approaches.",
                        "category": "algorithm",
                        "duration": 8,
                        "key_concepts": ["Recommendation systems", "Collaborative filtering", "Content-based filtering"],
                        "sample_answer_outline": ["Algorithm comparison", "Cold start problem", "Hybrid approaches"],
                        "difficulty_justification": "Applied ML system design"
                    },
                    {
                        "question": "Your machine learning model has high training accuracy but poor test performance. Diagnose the issue and propose solutions.",
                        "category": "debugging",
                        "duration": 6,
                        "key_concepts": ["Overfitting", "Model validation", "Regularization"],
                        "sample_answer_outline": ["Overfitting diagnosis", "Regularization techniques", "Cross-validation"],
                        "difficulty_justification": "Model debugging and optimization skills"
                    }
                ],
                "advanced": [
                    {
                        "question": "Design an A/B testing framework for a company with 100M users. Consider statistical power, multiple testing, and real-time analysis.",
                        "category": "system-design",
                        "duration": 12,
                        "key_concepts": ["A/B testing", "Statistical inference", "Experimentation platform"],
                        "sample_answer_outline": ["Statistical design", "Multiple testing correction", "Real-time monitoring"],
                        "difficulty_justification": "Advanced statistical system design"
                    }
                ]
            },
            "DevOps Engineer": {
                "beginner": [
                    {
                        "question": "Explain the difference between containers and virtual machines. What are the advantages of each?",
                        "category": "technical",
                        "duration": 5,
                        "key_concepts": ["Containerization", "Virtualization", "Docker"],
                        "sample_answer_outline": ["Resource usage differences", "Isolation levels", "Use cases"],
                        "difficulty_justification": "Fundamental containerization concept"
                    },
                    {
                        "question": "What is CI/CD and why is it important? Describe a basic CI/CD pipeline.",
                        "category": "technical",
                        "duration": 6,
                        "key_concepts": ["CI/CD", "Automation", "Deployment pipeline"],
                        "sample_answer_outline": ["Continuous integration", "Continuous deployment", "Pipeline stages"],
                        "difficulty_justification": "Core DevOps practice"
                    }
                ],
                "intermediate": [
                    {
                        "question": "Design a monitoring and alerting system for a microservices architecture. What metrics would you track?",
                        "category": "system-design",
                        "duration": 8,
                        "key_concepts": ["Monitoring", "Alerting", "Microservices observability"],
                        "sample_answer_outline": ["Key metrics", "Alert thresholds", "Distributed tracing"],
                        "difficulty_justification": "Microservices monitoring expertise"
                    },
                    {
                        "question": "Your production database is running out of storage space. Walk through your incident response process.",
                        "category": "debugging",
                        "duration": 7,
                        "key_concepts": ["Incident response", "Database management", "Capacity planning"],
                        "sample_answer_outline": ["Immediate actions", "Root cause analysis", "Prevention measures"],
                        "difficulty_justification": "Production incident handling"
                    }
                ],
                "advanced": [
                    {
                        "question": "Design a multi-region disaster recovery strategy for a critical financial application. Consider RTO, RPO, and compliance requirements.",
                        "category": "architecture",
                        "duration": 12,
                        "key_concepts": ["Disaster recovery", "Multi-region architecture", "Compliance"],
                        "sample_answer_outline": ["DR strategy", "Failover mechanisms", "Compliance considerations"],
                        "difficulty_justification": "Enterprise-level DR planning"
                    }
                ]
            },
            "Product Manager": {
                "beginner": [
                    {
                        "question": "How would you prioritize features in a product backlog? Describe your decision-making framework.",
                        "category": "technical",
                        "duration": 5,
                        "key_concepts": ["Feature prioritization", "Product backlog", "Decision frameworks"],
                        "sample_answer_outline": ["Prioritization methods", "Stakeholder input", "Business value assessment"],
                        "difficulty_justification": "Core PM skill"
                    }
                ],
                "intermediate": [
                    {
                        "question": "Design an API strategy for a platform that serves both internal teams and external developers. Consider versioning, documentation, and developer experience.",
                        "category": "system-design",
                        "duration": 8,
                        "key_concepts": ["API strategy", "Developer experience", "Platform design"],
                        "sample_answer_outline": ["API design principles", "Versioning strategy", "Developer onboarding"],
                        "difficulty_justification": "Technical PM API expertise"
                    }
                ],
                "advanced": [
                    {
                        "question": "You're launching a new product in a competitive market. Design a go-to-market strategy that includes technical considerations for scalability.",
                        "category": "architecture",
                        "duration": 10,
                        "key_concepts": ["Go-to-market strategy", "Technical scalability", "Product launch"],
                        "sample_answer_outline": ["Market analysis", "Technical architecture", "Launch phases"],
                        "difficulty_justification": "Strategic product leadership"
                    }
                ]
            }
        }
        
        # Get questions for the role and difficulty
        role_questions = technical_question_banks.get(main_role, technical_question_banks["Software Developer"])
        difficulty_questions = role_questions.get(difficulty, role_questions.get("intermediate", []))
        
        # Return requested number of questions, cycling through if needed
        if len(difficulty_questions) >= count:
            return difficulty_questions[:count]
        else:
            # Cycle through questions if we need more than available
            result = []
            for i in range(count):
                result.append(difficulty_questions[i % len(difficulty_questions)])
            return result

    def _get_fallback_debugging_questions(self, difficulty: str, count: int, language: str) -> List[Dict[str, Any]]:
        """Fallback debugging questions when AI generation fails"""
        
        debugging_questions = {
            "beginner": [
                {
                    "question": "Review this JavaScript function and identify the bug:\n```javascript\nfunction findMax(arr) {\n  let max = 0;\n  for (let i = 0; i < arr.length; i++) {\n    if (arr[i] > max) max = arr[i];\n  }\n  return max;\n}\n```",
                    "category": "debugging",
                    "duration": 4,
                    "key_concepts": ["Logic errors", "Edge cases", "Initialization"],
                    "sample_answer_outline": ["max initialized to 0", "Fails with negative numbers", "Should use arr[0] or -Infinity"]
                }
            ],
            "intermediate": [
                {
                    "question": "This API endpoint is experiencing memory leaks. Identify potential causes and solutions:\n```javascript\napp.get('/users', async (req, res) => {\n  const users = [];\n  const stream = db.createReadStream();\n  stream.on('data', (user) => users.push(user));\n  stream.on('end', () => res.json(users));\n});\n```",
                    "category": "debugging",
                    "duration": 6,
                    "key_concepts": ["Memory management", "Stream handling", "Error handling"],
                    "sample_answer_outline": ["Unbounded array growth", "Missing error handling", "Stream cleanup needed"]
                }
            ],
            "advanced": [
                {
                    "question": "Analyze this concurrent code for race conditions and suggest improvements:\n```java\npublic class Counter {\n  private int count = 0;\n  public void increment() { count++; }\n  public int getCount() { return count; }\n}\n```",
                    "category": "debugging",
                    "duration": 7,
                    "key_concepts": ["Concurrency", "Thread safety", "Synchronization"],
                    "sample_answer_outline": ["Non-atomic operations", "Race conditions", "Synchronization solutions"]
                }
            ]
        }
        
        questions = debugging_questions.get(difficulty, debugging_questions["intermediate"])
        return questions[:count] if len(questions) >= count else questions * ((count // len(questions)) + 1)[:count]
    
    def _extract_question_topics(self, question: str) -> set:
        """Extract key topics from a question to avoid repetition"""
        
        # Common technical and behavioral keywords to track
        keywords = {
            # Technical topics
            'database', 'sql', 'api', 'framework', 'algorithm', 'data structure',
            'testing', 'debugging', 'performance', 'security', 'architecture',
            'design pattern', 'microservices', 'cloud', 'deployment', 'ci/cd',
            
            # Behavioral topics  
            'leadership', 'teamwork', 'conflict', 'communication', 'project',
            'challenge', 'problem solving', 'decision making', 'time management',
            'feedback', 'mentoring', 'collaboration', 'priority', 'deadline'
        }
        
        question_lower = question.lower()
        found_topics = set()
        
        for keyword in keywords:
            if keyword in question_lower:
                found_topics.add(keyword)
        
        return found_topics
    
    def _extract_answer_insights(self, answer: str, question: str) -> List[str]:
        """Extract key insights from user's answer for follow-up question generation"""
        
        if not answer or len(answer.strip()) < 10:
            return []
        
        insights = []
        answer_lower = answer.lower()
        
        try:
            # Extract mentioned technologies
            tech_keywords = [
                'react', 'angular', 'vue', 'node.js', 'python', 'java', 'javascript',
                'typescript', 'mongodb', 'postgresql', 'mysql', 'redis', 'docker',
                'kubernetes', 'aws', 'azure', 'gcp', 'jenkins', 'git', 'github'
            ]
            
            mentioned_tech = [tech for tech in tech_keywords if tech in answer_lower]
            if mentioned_tech:
                insights.extend([f"mentioned {tech}" for tech in mentioned_tech[:3]])
            
            # Extract project references
            project_indicators = [
                'project', 'application', 'system', 'platform', 'website',
                'app', 'service', 'tool', 'framework', 'library'
            ]
            
            for indicator in project_indicators:
                if indicator in answer_lower:
                    insights.append(f"discussed {indicator} experience")
                    break
            
            # Extract experience indicators
            experience_indicators = [
                'implemented', 'developed', 'built', 'created', 'designed',
                'managed', 'led', 'worked on', 'responsible for', 'handled'
            ]
            
            for indicator in experience_indicators:
                if indicator in answer_lower:
                    insights.append(f"has {indicator} experience")
                    break
            
            # Extract challenge/problem references
            challenge_indicators = [
                'challenge', 'problem', 'issue', 'difficulty', 'obstacle',
                'bug', 'error', 'failure', 'bottleneck', 'performance issue'
            ]
            
            for indicator in challenge_indicators:
                if indicator in answer_lower:
                    insights.append(f"faced {indicator}")
                    break
            
            # Extract methodology references
            methodology_indicators = [
                'agile', 'scrum', 'kanban', 'waterfall', 'tdd', 'bdd',
                'ci/cd', 'devops', 'microservices', 'monolith'
            ]
            
            mentioned_methods = [method for method in methodology_indicators if method in answer_lower]
            if mentioned_methods:
                insights.extend([f"uses {method}" for method in mentioned_methods[:2]])
            
            # Extract team/collaboration references
            if any(word in answer_lower for word in ['team', 'colleague', 'collaborate', 'pair programming']):
                insights.append("has team collaboration experience")
            
            # Extract learning/growth references
            if any(word in answer_lower for word in ['learned', 'improved', 'grew', 'developed skills']):
                insights.append("shows learning and growth mindset")
            
            return insights[:5]  # Return top 5 insights
            
        except Exception as e:
            logger.error(f"Error extracting answer insights: {e}")
            return []
    
    def _get_role_validation_criteria(self, role: str) -> str:
        """Get role-specific validation criteria for question quality"""
        
        criteria_map = {
            "Software Developer": """
            - Questions must test specific programming concepts, not general problem-solving
            - Include scenarios about code review, debugging, or architecture decisions
            - Ask about specific technologies, frameworks, or development practices
            - Test understanding of software development lifecycle and best practices
            - Avoid questions that could be answered by any technical role
            """,
            
            "Data Scientist": """
            - Questions must involve data analysis, statistical concepts, or machine learning
            - Include scenarios about model selection, data preprocessing, or interpretation
            - Test understanding of statistical significance, bias, and validation methods
            - Ask about specific tools (Python/R), libraries, or methodologies
            - Avoid generic analytical questions that don't require data science expertise
            """,
            
            "Product Manager": """
            - Questions must involve product strategy, roadmapping, or stakeholder management
            - Include scenarios about feature prioritization, user research, or metrics
            - Test understanding of product lifecycle and go-to-market strategies
            - Ask about specific PM tools, frameworks, or methodologies
            - Avoid questions that could apply to any management role
            """,
            
            "DevOps Engineer": """
            - Questions must involve infrastructure, deployment, or operational concerns
            - Include scenarios about system reliability, monitoring, or incident response
            - Test understanding of CI/CD, containerization, or cloud platforms
            - Ask about specific tools (Docker, Kubernetes, Terraform) or practices
            - Avoid questions that don't require infrastructure expertise
            """,
            
            "UX/UI Designer": """
            - Questions must involve user experience, design process, or usability
            - Include scenarios about user research, prototyping, or design systems
            - Test understanding of design principles, accessibility, or user psychology
            - Ask about specific design tools, methodologies, or testing approaches
            - Avoid questions that could apply to any creative role
            """
        }
        
        return criteria_map.get(role, """
        - Questions must be specific to the role's core responsibilities
        - Include scenarios relevant to the position's daily challenges
        - Test role-specific knowledge, skills, and experience
        - Ask about industry-standard tools, practices, or methodologies
        - Avoid generic questions that could apply to any position
        """)
    
    def _validate_question_relevance(self, questions: List[Dict[str, Any]], role_context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Enhanced validation of generated questions against role requirements with hierarchical context"""
        
        if not role_context:
            return questions
        
        main_role = role_context.get('main_role', '')
        sub_role = role_context.get('sub_role', '')
        specialization = role_context.get('specialization', '')
        tech_stacks = role_context.get('tech_stacks', [])
        question_tags = role_context.get('question_tags', [])
        
        validated_questions = []
        
        # Get hierarchical role competencies for validation
        technical_competencies = self._get_technical_competencies(main_role, sub_role, specialization)
        
        for question_data in questions:
            question_text = question_data.get('question', '').lower()
            category = question_data.get('category', '')
            key_points = question_data.get('key_points', [])
            
            # Enhanced relevance scoring system
            relevance_score = 0
            max_score = 100
            
            # Technical questions validation (40 points max)
            if category == 'technical':
                # Check for specific tech stack mentions (20 points)
                tech_mentions = sum(1 for tech in tech_stacks if tech.lower() in question_text)
                relevance_score += min(tech_mentions * 5, 20)
                
                # Check for technical competency alignment (15 points)
                competency_matches = sum(1 for comp in technical_competencies 
                                       if any(word in question_text for word in comp.lower().split()))
                relevance_score += min(competency_matches * 3, 15)
                
                # Check for role-specific keywords (5 points)
                role_keywords = self._get_role_keywords(main_role)
                keyword_matches = sum(1 for keyword in role_keywords if keyword in question_text)
                relevance_score += min(keyword_matches * 1, 5)
            
            # Behavioral/situational questions validation (30 points max)
            elif category in ['behavioral', 'situational']:
                # Check for role-specific scenarios (20 points)
                role_scenarios = self._get_role_scenarios(main_role)
                scenario_matches = sum(1 for scenario in role_scenarios if scenario in question_text)
                relevance_score += min(scenario_matches * 10, 20)
                
                # Check for role context in question (10 points)
                role_context_words = [main_role.lower(), sub_role.lower() if sub_role else '']
                context_matches = sum(1 for word in role_context_words if word and word in question_text)
                relevance_score += min(context_matches * 5, 10)
            
            # System design and problem-solving validation (25 points max)
            elif category in ['system-design', 'problem-solving']:
                # Check for architecture/design relevance (15 points)
                design_keywords = ['architecture', 'design', 'system', 'scalability', 'performance']
                design_matches = sum(1 for keyword in design_keywords if keyword in question_text)
                relevance_score += min(design_matches * 3, 15)
                
                # Check for role-appropriate complexity (10 points)
                if len(question_text) > 100:  # Complex questions are good for these categories
                    relevance_score += 10
            
            # General quality validation (30 points max)
            # Question length and detail (10 points)
            if len(question_text) > 50:
                relevance_score += 10
            elif len(question_text) > 30:
                relevance_score += 5
            
            # Key points alignment (10 points)
            if key_points and len(key_points) >= 2:
                relevance_score += 10
            elif key_points and len(key_points) >= 1:
                relevance_score += 5
            
            # Hierarchical context usage (10 points)
            hierarchical_context = question_data.get('hierarchical_context', '')
            if hierarchical_context and len(hierarchical_context) > 20:
                relevance_score += 10
            
            # Question tags alignment (bonus points)
            if question_tags:
                tag_matches = sum(1 for tag in question_tags 
                                if tag.lower().replace('-', ' ') in question_text)
                relevance_score += min(tag_matches * 2, 10)
            
            # Determine if question passes validation (threshold: 40/100)
            relevance_threshold = 40
            is_relevant = relevance_score >= relevance_threshold
            
            # Additional quality checks
            if is_relevant:
                # Ensure question is not too generic
                generic_indicators = ['tell me about', 'describe your', 'what is your experience']
                is_generic = any(indicator in question_text for indicator in generic_indicators)
                
                if is_generic and relevance_score < 60:
                    is_relevant = False
                    logger.debug(f"Filtered generic question (score: {relevance_score}): {question_text[:50]}...")
                
                # Ensure technical questions have sufficient depth
                if category == 'technical' and relevance_score < 50:
                    is_relevant = False
                    logger.debug(f"Filtered shallow technical question (score: {relevance_score}): {question_text[:50]}...")
            
            if is_relevant:
                # Add relevance score to question data for potential sorting
                question_data['relevance_score'] = relevance_score
                validated_questions.append(question_data)
            else:
                logger.debug(f"Filtered irrelevant question (score: {relevance_score}): {question_text[:50]}...")
        
        # Sort by relevance score (highest first) to prioritize best questions
        validated_questions.sort(key=lambda q: q.get('relevance_score', 0), reverse=True)
        
        return validated_questions
    
    def _validate_contextual_relevance(
        self, 
        questions: List[Dict[str, Any]], 
        previous_question: str, 
        user_answer: str, 
        role_context: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """Validate that follow-up questions are contextually relevant to the previous Q&A"""
        
        if not questions or not previous_question or not user_answer:
            return questions
        
        validated_questions = []
        
        # Extract key elements from previous question and answer
        prev_question_topics = self._extract_question_topics(previous_question)
        answer_insights = self._extract_answer_insights(user_answer, previous_question)
        
        # Get role-specific validation criteria
        main_role = role_context.get('main_role', '') if role_context else ''
        
        for question_data in questions:
            question_text = question_data.get('question', '').lower()
            category = question_data.get('category', '')
            
            # Calculate contextual relevance score
            relevance_score = 0
            
            # Check if follow-up builds on previous question topics (30 points)
            topic_connections = sum(1 for topic in prev_question_topics 
                                  if topic in question_text)
            relevance_score += min(topic_connections * 10, 30)
            
            # Check if follow-up references answer insights (40 points)
            insight_connections = sum(1 for insight in answer_insights 
                                    if any(word in question_text for word in insight.split()))
            relevance_score += min(insight_connections * 8, 40)
            
            # Check for natural conversation flow indicators (20 points)
            flow_indicators = [
                'you mentioned', 'you said', 'you described', 'building on',
                'following up', 'can you elaborate', 'tell me more about',
                'how did you', 'what was your approach', 'walk me through'
            ]
            
            flow_matches = sum(1 for indicator in flow_indicators if indicator in question_text)
            relevance_score += min(flow_matches * 5, 20)
            
            # Check for specific references to user's answer content (10 points)
            # Look for specific words/phrases from the user's answer
            answer_words = set(word.lower() for word in user_answer.split() 
                             if len(word) > 4 and word.isalpha())
            
            specific_references = sum(1 for word in answer_words if word in question_text)
            relevance_score += min(specific_references * 2, 10)
            
            # Contextual relevance threshold (minimum 25/100)
            contextual_threshold = 25
            
            # Additional quality checks for follow-up questions
            if relevance_score >= contextual_threshold:
                # Ensure question is not just repeating the previous question
                question_similarity = self._calculate_similarity(
                    question_text, previous_question.lower()
                )
                
                if question_similarity > 0.7:
                    logger.debug(f"Filtered repetitive follow-up question: {question_text[:50]}...")
                    continue
                
                # Ensure question adds value and depth
                if len(question_text) < 20:
                    logger.debug(f"Filtered shallow follow-up question: {question_text[:50]}...")
                    continue
                
                # Add contextual relevance score
                question_data['contextual_relevance_score'] = relevance_score
                validated_questions.append(question_data)
            else:
                logger.debug(f"Filtered contextually irrelevant follow-up (score: {relevance_score}): {question_text[:50]}...")
        
        # Sort by contextual relevance score
        validated_questions.sort(key=lambda q: q.get('contextual_relevance_score', 0), reverse=True)
        
        return validated_questions
    
    def _create_intelligent_followup(
        self, 
        previous_question: str, 
        user_answer: str, 
        role: str, 
        role_context: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """Create intelligent follow-up questions when AI generation fails"""
        
        try:
            # Extract insights from the user's answer
            answer_insights = self._extract_answer_insights(user_answer, previous_question)
            
            # Create contextual follow-up questions based on answer analysis
            followup_questions = []
            
            # Strategy 1: Ask for specific examples if they mentioned general concepts
            if any('experience' in insight for insight in answer_insights):
                followup_questions.append({
                    "question": f"Can you walk me through a specific example where you applied this approach?",
                    "category": "behavioral",
                    "duration": 4,
                    "key_points": ["specific example", "detailed process", "concrete results"],
                    "answer_connection": "Exploring specific implementation of mentioned experience"
                })
            
            # Strategy 2: Dive deeper into mentioned technologies
            tech_mentions = [insight for insight in answer_insights if 'mentioned' in insight]
            if tech_mentions:
                tech_name = tech_mentions[0].replace('mentioned ', '')
                followup_questions.append({
                    "question": f"What specific challenges did you face when working with {tech_name}, and how did you overcome them?",
                    "category": "technical",
                    "duration": 4,
                    "key_points": ["technical challenges", "problem-solving approach", "solution implementation"],
                    "answer_connection": f"Exploring depth of {tech_name} experience"
                })
            
            # Strategy 3: Explore decision-making process
            if any('implemented' in insight or 'developed' in insight for insight in answer_insights):
                followup_questions.append({
                    "question": "What factors influenced your decision-making process in this situation?",
                    "category": "problem-solving",
                    "duration": 3,
                    "key_points": ["decision criteria", "trade-off analysis", "reasoning process"],
                    "answer_connection": "Understanding decision-making approach"
                })
            
            # Strategy 4: Scale and complexity questions
            if len(user_answer.split()) > 30:  # Detailed answer suggests experience
                followup_questions.append({
                    "question": "How would your approach change if you had to scale this solution for a much larger system?",
                    "category": "system-design",
                    "duration": 5,
                    "key_points": ["scalability considerations", "architectural changes", "performance optimization"],
                    "answer_connection": "Testing scalability thinking"
                })
            
            # Strategy 5: Learning and growth
            if any('challenge' in insight or 'problem' in insight for insight in answer_insights):
                followup_questions.append({
                    "question": "What did you learn from this experience, and how has it influenced your approach since then?",
                    "category": "behavioral",
                    "duration": 3,
                    "key_points": ["learning outcomes", "skill development", "process improvement"],
                    "answer_connection": "Exploring growth mindset and learning"
                })
            
            # If no specific insights, create general but thoughtful follow-ups
            if not followup_questions:
                followup_questions = [
                    {
                        "question": "Can you describe a specific challenge you faced in a similar situation and how you approached it?",
                        "category": "behavioral",
                        "duration": 4,
                        "key_points": ["problem identification", "solution approach", "outcome"],
                        "answer_connection": "Exploring problem-solving in similar contexts"
                    },
                    {
                        "question": "What would you do differently if you encountered this situation again?",
                        "category": "behavioral",
                        "duration": 3,
                        "key_points": ["reflection", "improvement thinking", "lessons learned"],
                        "answer_connection": "Testing reflective thinking and continuous improvement"
                    }
                ]
            
            # Limit to 2 follow-up questions and add role context if available
            selected_questions = followup_questions[:2]
            
            # Enhance questions with role context if available
            if role_context:
                main_role = role_context.get('main_role', '')
                for question in selected_questions:
                    question['role_relevance'] = f"Relevant for {main_role} role assessment"
                    question['hierarchical_context'] = f"Contextual follow-up for {main_role} interview"
            
            logger.info(f"Created {len(selected_questions)} intelligent follow-up questions")
            return selected_questions
            
        except Exception as e:
            logger.error(f"Error creating intelligent follow-up: {e}")
            # Return basic fallback questions
            return [
                {
                    "question": "Can you provide more details about your experience with this?",
                    "category": "behavioral",
                    "duration": 3,
                    "key_points": ["detailed experience", "specific examples"],
                    "answer_connection": "General follow-up for more detail"
                }
            ]
    
    def _get_role_keywords(self, role: str) -> List[str]:
        """Get role-specific keywords that should appear in technical questions"""
        
        keyword_map = {
            "Software Developer": [
                "code", "programming", "algorithm", "database", "api", "framework",
                "testing", "debugging", "architecture", "design pattern", "performance"
            ],
            "Data Scientist": [
                "data", "model", "algorithm", "statistical", "machine learning", "analysis",
                "dataset", "feature", "prediction", "validation", "visualization"
            ],
            "Product Manager": [
                "product", "feature", "user", "roadmap", "stakeholder", "metrics",
                "requirements", "priority", "market", "strategy", "launch"
            ],
            "DevOps Engineer": [
                "infrastructure", "deployment", "pipeline", "monitoring", "server",
                "container", "cloud", "automation", "scaling", "reliability"
            ]
        }
        
        return keyword_map.get(role, [])
    
    def _get_role_scenarios(self, role: str) -> List[str]:
        """Get role-specific scenarios that should appear in behavioral questions"""
        
        scenario_map = {
            "Software Developer": [
                "development team", "code review", "technical debt", "project deadline",
                "bug fix", "feature implementation", "system design", "performance issue"
            ],
            "Data Scientist": [
                "data analysis", "model performance", "stakeholder presentation", "data quality",
                "research project", "experiment design", "model deployment", "business insight"
            ],
            "Product Manager": [
                "product launch", "feature prioritization", "user feedback", "cross-functional team",
                "market research", "product strategy", "stakeholder alignment", "roadmap planning"
            ],
            "DevOps Engineer": [
                "system outage", "deployment pipeline", "infrastructure scaling", "monitoring alert",
                "security incident", "performance optimization", "automation project", "disaster recovery"
            ]
        }
        
        return scenario_map.get(role, [])

    def _create_generic_followup(
        self, 
        previous_question: str, 
        user_answer: str, 
        role: str
    ) -> List[Dict[str, Any]]:
        """Create generic follow-up questions when AI generation fails"""
        
        # Analyze the answer for keywords to create relevant follow-ups
        answer_lower = user_answer.lower()
        
        followups = []
        
        # Check for common topics and create relevant follow-ups
        if any(word in answer_lower for word in ['project', 'projects', 'worked on', 'built', 'developed']):
            followups.append({
                "question": "Can you tell me more about one specific project you mentioned?",
                "category": "behavioral",
                "duration": 4,
                "key_points": ["Project details", "Your role", "Challenges faced", "Results achieved"]
            })
        
        if any(word in answer_lower for word in ['team', 'collaborate', 'worked with', 'colleagues']):
            followups.append({
                "question": "How do you typically handle collaboration and communication in team settings?",
                "category": "behavioral", 
                "duration": 3,
                "key_points": ["Communication style", "Conflict resolution", "Team dynamics"]
            })
        
        if any(word in answer_lower for word in ['challenge', 'difficult', 'problem', 'issue']):
            followups.append({
                "question": "Walk me through how you approach problem-solving when facing technical challenges.",
                "category": "behavioral",
                "duration": 4,
                "key_points": ["Problem analysis", "Solution approach", "Learning outcomes"]
            })
        
        if any(word in answer_lower for word in ['technology', 'tech', 'programming', 'coding', 'development']):
            followups.append({
                "question": f"What specific technologies or tools do you prefer working with in {role} roles?",
                "category": "technical",
                "duration": 3,
                "key_points": ["Technology preferences", "Experience level", "Learning approach"]
            })
        
        # If no specific topics found, use generic follow-ups
        if not followups:
            followups = [
                {
                    "question": "Can you elaborate on that with a specific example?",
                    "category": "behavioral",
                    "duration": 3,
                    "key_points": ["Specific example", "Context", "Your actions", "Results"]
                },
                {
                    "question": "What did you learn from that experience?",
                    "category": "behavioral",
                    "duration": 2,
                    "key_points": ["Key learnings", "Personal growth", "Application to future"]
                }
            ]
        
        return followups[:2]  # Return max 2 follow-ups
    
    def generate_comprehensive_feedback(self, performance_data: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive feedback with specific recommendations and areas for improvement"""
        
        try:
            logger.info("Generating comprehensive feedback using Gemini API")
            
            prompt = self._build_comprehensive_feedback_prompt(performance_data)
            
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
                "max_output_tokens": 3072,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            feedback = self._parse_comprehensive_feedback_response(response_text)
            
            if feedback:
                logger.info("Successfully generated comprehensive feedback")
                return feedback
            else:
                logger.warning("Failed to parse comprehensive feedback, using fallback")
                return self._get_fallback_comprehensive_feedback(performance_data)
                
        except Exception as e:
            logger.error(f"Error generating comprehensive feedback: {str(e)}")
            return self._get_fallback_comprehensive_feedback(performance_data)
    
    def generate_database_questions(
        self,
        role_data: Dict[str, Any],
        difficulty: str = "intermediate",
        count: int = 3,
        database_type: str = "mixed"
    ) -> List[Dict[str, Any]]:
        """Generate database-specific technical questions"""
        
        try:
            logger.info(f"Generating database questions for {self._get_role_attribute(role_data, 'main_role', 'Unknown')}")
            
            prompt = self._build_database_question_prompt(role_data, difficulty, count, database_type)
            
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
                "max_output_tokens": 3072,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            questions_data = self._parse_technical_questions_response(response_text)
            
            if questions_data:
                logger.info(f"Generated {len(questions_data)} database questions")
                return questions_data
            else:
                logger.warning("Failed to generate database questions, using fallback")
                return self._get_fallback_database_questions(difficulty, count, database_type)
                
        except Exception as e:
            logger.error(f"Error generating database questions: {str(e)}")
            return self._get_fallback_database_questions(difficulty, count, database_type)
    
    def generate_algorithm_questions(
        self,
        role_data: Dict[str, Any],
        difficulty: str = "intermediate",
        count: int = 3,
        focus_area: str = "mixed"
    ) -> List[Dict[str, Any]]:
        """Generate algorithm and data structure questions"""
        
        try:
            logger.info(f"Generating algorithm questions for {self._get_role_attribute(role_data, 'main_role', 'Unknown')}")
            
            prompt = self._build_algorithm_question_prompt(role_data, difficulty, count, focus_area)
            
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
                "max_output_tokens": 3072,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            questions_data = self._parse_technical_questions_response(response_text)
            
            if questions_data:
                logger.info(f"Generated {len(questions_data)} algorithm questions")
                return questions_data
            else:
                logger.warning("Failed to generate algorithm questions, using fallback")
                return self._get_fallback_algorithm_questions(difficulty, count, focus_area)
                
        except Exception as e:
            logger.error(f"Error generating algorithm questions: {str(e)}")
            return self._get_fallback_algorithm_questions(difficulty, count, focus_area)
    
    def generate_system_design_questions(
        self,
        role_data: Dict[str, Any],
        difficulty: str = "intermediate",
        count: int = 2,
        system_type: str = "web_application"
    ) -> List[Dict[str, Any]]:
        """Generate system design and architecture questions"""
        
        try:
            logger.info(f"Generating system design questions for {self._get_role_attribute(role_data, 'main_role', 'Unknown')}")
            
            prompt = self._build_system_design_question_prompt(role_data, difficulty, count, system_type)
            
            generation_config = {
                "temperature": 0.8,
                "top_p": 0.9,
                "top_k": 40,
                "max_output_tokens": 3072,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            questions_data = self._parse_technical_questions_response(response_text)
            
            if questions_data:
                logger.info(f"Generated {len(questions_data)} system design questions")
                return questions_data
            else:
                logger.warning("Failed to generate system design questions, using fallback")
                return self._get_fallback_system_design_questions(difficulty, count, system_type)
                
        except Exception as e:
            logger.error(f"Error generating system design questions: {str(e)}")
            return self._get_fallback_system_design_questions(difficulty, count, system_type)
    
    def generate_debugging_questions(
        self,
        role_data: Dict[str, Any],
        difficulty: str = "intermediate",
        count: int = 2,
        language: str = "javascript"
    ) -> List[Dict[str, Any]]:
        """Generate code debugging and review questions"""
        
        try:
            logger.info(f"Generating debugging questions for {self._get_role_attribute(role_data, 'main_role', 'Unknown')}")
            
            prompt = self._build_debugging_question_prompt(role_data, difficulty, count, language)
            
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
                "max_output_tokens": 3072,
            }
            
            response = self.model.generate_content(prompt, generation_config=generation_config)
            
            response_text = ""
            if hasattr(response, 'text') and response.text:
                response_text = response.text
            elif hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    response_text = response.candidates[0].content.parts[0].text
            
            questions_data = self._parse_technical_questions_response(response_text)
            
            if questions_data:
                logger.info(f"Generated {len(questions_data)} debugging questions")
                return questions_data
            else:
                logger.warning("Failed to generate debugging questions, using fallback")
                return self._get_fallback_debugging_questions(difficulty, count, language)
                
        except Exception as e:
            logger.error(f"Error generating debugging questions: {str(e)}")
            return self._get_fallback_debugging_questions(difficulty, count, language)
    
    def _build_database_question_prompt(
        self,
        role_data: Dict[str, Any],
        difficulty: str,
        count: int,
        database_type: str
    ) -> str:
        """Build prompt for database question generation"""
        
        main_role = self._get_role_attribute(role_data, 'main_role', '')
        sub_role = self._get_role_attribute(role_data, 'sub_role', '')
        tech_stacks = self._get_role_attribute(role_data, 'tech_stacks', [])
        
        database_focus = {
            "sql": "SQL databases (PostgreSQL, MySQL, SQL Server) - focus on complex queries, optimization, and schema design",
            "nosql": "NoSQL databases (MongoDB, Redis, Cassandra) - focus on document/key-value operations and scaling",
            "mixed": "Both SQL and NoSQL databases - cover relational and non-relational data modeling"
        }
        
        difficulty_specs = {
            "beginner": "Basic CRUD operations, simple joins, and fundamental concepts",
            "intermediate": "Complex queries, performance optimization, and database design",
            "advanced": "Advanced optimization, sharding, replication, and distributed database concepts"
        }
        
        prompt = f"""
        You are a senior database architect creating technical interview questions.
        
        ROLE CONTEXT:
        - Main Role: {main_role}
        - Sub Role: {sub_role}
        - Technologies: {', '.join(tech_stacks[:3]) if tech_stacks else 'Standard databases'}
        
        DATABASE FOCUS: {database_focus.get(database_type, database_focus['mixed'])}
        DIFFICULTY: {difficulty_specs.get(difficulty, difficulty_specs['intermediate'])}
        
        Generate {count} database questions that test:
        
        FOR BEGINNERS:
        - Basic SQL syntax and CRUD operations
        - Simple JOIN operations
        - Basic indexing concepts
        - Data types and constraints
        
        FOR INTERMEDIATE:
        - Complex JOIN queries with multiple tables
        - Query optimization and EXPLAIN plans
        - Database schema design and normalization
        - Performance tuning and indexing strategies
        - Stored procedures and functions
        
        FOR ADVANCED:
        - Database sharding and partitioning
        - Replication and consistency models
        - Distributed database concepts
        - Advanced optimization techniques
        - Database security and compliance
        
        Each question should include:
        1. A realistic scenario or problem
        2. Specific database context (table structures, data volumes, etc.)
        3. Clear technical requirements
        4. Expected solution approach
        
        Format as JSON:
        [
            {{
                "question": "Detailed database question with specific scenario and requirements",
                "category": "database",
                "duration": 6,
                "key_concepts": ["SQL optimization", "Indexing", "Performance tuning"],
                "sample_answer_outline": ["Query analysis", "Index strategy", "Performance metrics"],
                "difficulty_justification": "Tests {difficulty} level database skills"
            }}
        ]
        
        Generate database questions now:
        """
        
        return prompt
    
    def _build_algorithm_question_prompt(
        self,
        role_data: Dict[str, Any],
        difficulty: str,
        count: int,
        focus_area: str
    ) -> str:
        """Build prompt for algorithm question generation"""
        
        main_role = self._get_role_attribute(role_data, 'main_role', '')
        sub_role = self._get_role_attribute(role_data, 'sub_role', '')
        
        focus_areas = {
            "arrays": "Array manipulation, searching, and sorting algorithms",
            "trees": "Binary trees, BSTs, tree traversal, and tree-based algorithms",
            "graphs": "Graph algorithms, shortest path, DFS/BFS, and network problems",
            "dynamic_programming": "DP problems, optimization, and memoization techniques",
            "mixed": "Variety of data structures and algorithmic approaches"
        }
        
        difficulty_specs = {
            "beginner": "Basic algorithms with clear solutions and O(n) or O(n log n) complexity",
            "intermediate": "Moderate complexity problems requiring algorithmic thinking and optimization",
            "advanced": "Complex algorithms, advanced data structures, and optimization challenges"
        }
        
        prompt = f"""
        You are a senior software engineer creating algorithmic interview questions.
        
        ROLE CONTEXT:
        - Main Role: {main_role}
        - Sub Role: {sub_role}
        
        ALGORITHM FOCUS: {focus_areas.get(focus_area, focus_areas['mixed'])}
        DIFFICULTY: {difficulty_specs.get(difficulty, difficulty_specs['intermediate'])}
        
        Generate {count} algorithm questions that test:
        
        FOR BEGINNERS:
        - Basic array and string manipulation
        - Simple searching and sorting
        - Basic recursion
        - Time/space complexity analysis
        
        FOR INTERMEDIATE:
        - Data structure selection and usage
        - Algorithm optimization
        - Dynamic programming basics
        - Graph traversal algorithms
        
        FOR ADVANCED:
        - Complex algorithmic problems
        - Advanced data structures (heaps, tries, etc.)
        - Optimization techniques
        - System-level algorithm design
        
        Each question should:
        1. Present a clear problem statement
        2. Include input/output examples
        3. Specify constraints and edge cases
        4. Test algorithmic thinking, not just coding syntax
        
        Format as JSON:
        [
            {{
                "question": "Clear algorithm problem with examples and constraints",
                "category": "algorithm",
                "duration": 7,
                "key_concepts": ["Data structures", "Algorithm design", "Complexity analysis"],
                "sample_answer_outline": ["Approach explanation", "Implementation strategy", "Complexity analysis"],
                "difficulty_justification": "Tests {difficulty} level algorithmic thinking"
            }}
        ]
        
        Generate algorithm questions now:
        """
        
        return prompt
    
    def _build_system_design_question_prompt(
        self,
        role_data: Dict[str, Any],
        difficulty: str,
        count: int,
        system_type: str
    ) -> str:
        """Build prompt for system design question generation"""
        
        main_role = self._get_role_attribute(role_data, 'main_role', '')
        sub_role = self._get_role_attribute(role_data, 'sub_role', '')
        tech_stacks = self._get_role_attribute(role_data, 'tech_stacks', [])
        
        system_types = {
            "web_application": "Web applications, APIs, and client-server architectures",
            "distributed_system": "Distributed systems, microservices, and scalability challenges",
            "data_system": "Data processing systems, ETL pipelines, and analytics platforms",
            "real_time": "Real-time systems, streaming, and low-latency applications"
        }
        
        difficulty_specs = {
            "beginner": "Simple system components and basic architecture decisions",
            "intermediate": "Scalable systems with multiple components and trade-off analysis",
            "advanced": "Complex distributed systems with advanced scalability and reliability requirements"
        }
        
        prompt = f"""
        You are a principal architect creating system design interview questions.
        
        ROLE CONTEXT:
        - Main Role: {main_role}
        - Sub Role: {sub_role}
        - Technologies: {', '.join(tech_stacks[:3]) if tech_stacks else 'Modern tech stack'}
        
        SYSTEM TYPE: {system_types.get(system_type, system_types['web_application'])}
        DIFFICULTY: {difficulty_specs.get(difficulty, difficulty_specs['intermediate'])}
        
        Generate {count} system design questions that test:
        
        FOR BEGINNERS:
        - Basic system components and interactions
        - Simple database and API design
        - Basic scalability concepts
        - Technology selection rationale
        
        FOR INTERMEDIATE:
        - Multi-component system architecture
        - Load balancing and caching strategies
        - Database design and optimization
        - API design and integration patterns
        
        FOR ADVANCED:
        - Large-scale distributed systems
        - Advanced scalability patterns
        - Consistency and availability trade-offs
        - Performance optimization at scale
        
        Each question should:
        1. Present a realistic business scenario
        2. Specify scale requirements (users, data, requests)
        3. Include constraints and requirements
        4. Test architectural thinking and trade-off analysis
        
        Format as JSON:
        [
            {{
                "question": "Detailed system design challenge with scale and requirements",
                "category": "system-design",
                "duration": 10,
                "key_concepts": ["System architecture", "Scalability", "Trade-off analysis"],
                "sample_answer_outline": ["Architecture overview", "Component design", "Scalability strategy"],
                "difficulty_justification": "Tests {difficulty} level system design skills"
            }}
        ]
        
        Generate system design questions now:
        """
        
        return prompt
    
    def _build_debugging_question_prompt(
        self,
        role_data: Dict[str, Any],
        difficulty: str,
        count: int,
        language: str
    ) -> str:
        """Build prompt for debugging question generation"""
        
        main_role = self._get_role_attribute(role_data, 'main_role', '')
        sub_role = self._get_role_attribute(role_data, 'sub_role', '')
        tech_stacks = self._get_role_attribute(role_data, 'tech_stacks', [])
        
        # Determine primary language from tech stacks or use provided language
        primary_language = language
        if tech_stacks:
            language_map = {
                'React': 'javascript', 'Vue': 'javascript', 'Angular': 'javascript',
                'Node.js': 'javascript', 'Express': 'javascript',
                'Python': 'python', 'Django': 'python', 'Flask': 'python',
                'Java': 'java', 'Spring': 'java',
                'C#': 'csharp', '.NET': 'csharp'
            }
            for tech in tech_stacks:
                if tech in language_map:
                    primary_language = language_map[tech]
                    break
        
        difficulty_specs = {
            "beginner": "Simple logic errors, syntax issues, and basic debugging techniques",
            "intermediate": "Complex bugs, performance issues, and code review scenarios",
            "advanced": "Subtle bugs, concurrency issues, and architectural problems"
        }
        
        prompt = f"""
        You are a senior developer creating code debugging interview questions.
        
        ROLE CONTEXT:
        - Main Role: {main_role}
        - Sub Role: {sub_role}
        - Primary Language: {primary_language}
        - Technologies: {', '.join(tech_stacks[:3]) if tech_stacks else 'Standard tools'}
        
        DIFFICULTY: {difficulty_specs.get(difficulty, difficulty_specs['intermediate'])}
        
        Generate {count} debugging questions that test:
        
        FOR BEGINNERS:
        - Basic syntax and logic errors
        - Simple debugging techniques
        - Code reading and understanding
        - Basic best practices
        
        FOR INTERMEDIATE:
        - Complex logic bugs
        - Performance issues
        - Memory leaks and resource management
        - Code review and refactoring
        
        FOR ADVANCED:
        - Subtle and hard-to-find bugs
        - Concurrency and race conditions
        - Security vulnerabilities
        - Architectural and design issues
        
        Each question should:
        1. Present actual buggy code (10-30 lines)
        2. Describe the symptoms or expected behavior
        3. Include context about the system/application
        4. Test debugging methodology, not just bug identification
        
        Format as JSON:
        [
            {{
                "question": "Code debugging scenario with buggy code snippet and context",
                "category": "debugging",
                "duration": 6,
                "key_concepts": ["Bug identification", "Debugging methodology", "Code review"],
                "sample_answer_outline": ["Bug identification", "Root cause analysis", "Fix implementation"],
                "difficulty_justification": "Tests {difficulty} level debugging skills"
            }}
        ]
        
        Generate debugging questions now:
        """
        
        return prompt
    
    def _build_comprehensive_feedback_prompt(self, performance_data: Dict[str, Any]) -> str:
        """Build prompt for comprehensive feedback generation"""
        
        session_info = performance_data.get('session_info', {})
        user_info = performance_data.get('user_info', {})
        scores = performance_data.get('performance_scores', {})
        qa_data = performance_data.get('questions_and_answers', [])
        
        # Build questions and answers section
        qa_section = ""
        for i, qa in enumerate(qa_data, 1):
            qa_section += f"""
Question {i}: {qa['question']}
Answer: {qa['answer']}
Content Score: {qa['content_score']}/100
Body Language Score: {qa['body_language_score']}/100
Tone Score: {qa['tone_score']}/100
"""
        
        prompt = f"""
You are an expert interview coach and career advisor. Analyze this interview performance and provide comprehensive, actionable feedback.

INTERVIEW SESSION DETAILS:
- Target Role: {session_info.get('target_role', 'Unknown')}
- Session Type: {session_info.get('session_type', 'mixed')}
- Duration: {session_info.get('duration', 30)} minutes
- Questions Answered: {session_info.get('questions_answered', 0)}

CANDIDATE PROFILE:
- Current Role: {user_info.get('role', 'Unknown')}
- Experience Level: {user_info.get('experience_level', 'intermediate')}

PERFORMANCE SCORES:
- Overall Score: {scores.get('overall_score', 0):.1f}/100
- Content Quality: {scores.get('content_quality', 0):.1f}/100
- Body Language: {scores.get('body_language', 0):.1f}/100
- Voice & Tone: {scores.get('voice_tone', 0):.1f}/100

QUESTIONS AND ANSWERS:
{qa_section}

Please provide detailed feedback in the following JSON format:
{{
    "areas_for_improvement": [
        "Specific area 1 with actionable advice",
        "Specific area 2 with actionable advice",
        "Specific area 3 with actionable advice"
    ],
    "recommendations": [
        "Specific recommendation 1 with clear action steps",
        "Specific recommendation 2 with clear action steps",
        "Specific recommendation 3 with clear action steps"
    ],
    "detailed_analysis": "A comprehensive paragraph analyzing the overall performance, highlighting strengths and weaknesses with specific examples from the answers provided.",
    "question_feedback": [
        {{
            "question": "Question text",
            "feedback": "Specific feedback on this answer",
            "improvement_tip": "How to improve this type of answer"
        }}
    ]
}}

GUIDELINES:
1. Be specific and actionable in your feedback
2. Reference actual answers when possible
3. Provide concrete steps for improvement
4. Consider the target role and session type
5. Balance constructive criticism with encouragement
6. Focus on skills like technical knowledge, communication, STAR method, confidence, etc.
7. For areas of improvement, be specific about what skills need work (e.g., "database optimization knowledge", "behavioral storytelling", "technical explanation clarity")
8. For recommendations, provide specific actions (e.g., "Practice explaining technical concepts to non-technical audiences", "Prepare 3-5 STAR method examples for behavioral questions")

Generate comprehensive feedback now:
"""
        
        return prompt
    
    def _parse_comprehensive_feedback_response(self, response_text: str) -> Dict[str, Any]:
        """Parse comprehensive feedback response from Gemini"""
        try:
            # Clean the response text
            cleaned_text = response_text.strip()
            if cleaned_text.startswith('```json'):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith('```'):
                cleaned_text = cleaned_text[:-3]
            
            feedback = json.loads(cleaned_text)
            
            # Validate required fields
            required_fields = ['areas_for_improvement', 'recommendations', 'detailed_analysis']
            for field in required_fields:
                if field not in feedback:
                    feedback[field] = []
            
            return feedback
            
        except Exception as e:
            logger.error(f"Error parsing comprehensive feedback response: {str(e)}")
            return None
    
    def _get_fallback_comprehensive_feedback(self, performance_data: Dict[str, Any]) -> Dict[str, Any]:
        """Generate fallback comprehensive feedback when AI fails"""
        
        scores = performance_data.get('performance_scores', {})
        session_info = performance_data.get('session_info', {})
        
        # Generate basic feedback based on scores
        areas_for_improvement = []
        recommendations = []
        
        content_score = scores.get('content_quality', 0)
        body_language_score = scores.get('body_language', 0)
        tone_score = scores.get('voice_tone', 0)
        
        if content_score < 70:
            areas_for_improvement.append("Content quality and depth of answers need improvement")
            recommendations.append("Practice answering questions with more specific examples and details")
        
        if body_language_score < 70:
            areas_for_improvement.append("Body language and non-verbal communication")
            recommendations.append("Practice maintaining good posture and eye contact during interviews")
        
        if tone_score < 70:
            areas_for_improvement.append("Voice tone and confidence level")
            recommendations.append("Work on speaking clearly and confidently, practice vocal exercises")
        
        # Add session-specific recommendations
        session_type = session_info.get('session_type', 'mixed')
        if session_type == 'technical':
            recommendations.append("Focus on practicing technical problem-solving and explanation skills")
        elif session_type == 'behavioral':
            recommendations.append("Prepare more STAR method examples for behavioral questions")
        elif session_type == 'hr':
            recommendations.append("Research common HR questions and practice professional responses")
        
        return {
            'areas_for_improvement': areas_for_improvement or ['Continue practicing to improve overall interview performance'],
            'recommendations': recommendations or ['Keep practicing regularly to build confidence and skills'],
            'detailed_analysis': f"Based on your {session_info.get('session_type', 'interview')} session for {session_info.get('target_role', 'the target role')}, you scored {scores.get('overall_score', 0):.1f}/100 overall. Focus on the specific areas mentioned above to improve your performance.",
            'question_feedback': []
        }
    
    def _is_cached(self, cache_key: str) -> bool:
        """Check if data is cached and not expired"""
        if cache_key not in self.cache:
            return False
        
        cached_time = self.cache[cache_key]["timestamp"]
        return datetime.now() - cached_time < self.cache_ttl
    
    def _cache_data(self, cache_key: str, data: Any):
        """Cache data with timestamp"""
        self.cache[cache_key] = {
            "data": data,
            "timestamp": datetime.now()
        }
    
    def _get_fallback_questions(
        self, 
        role: str, 
        difficulty: str, 
        question_type: str, 
        count: int
    ) -> List[Dict[str, Any]]:
        """Return comprehensive fallback questions when API fails"""
        
        # Role-specific question templates
        role_questions = {
            'Software Developer': [
                {
                    "question": "Can you walk me through your experience with object-oriented programming?",
                    "category": "technical",
                    "duration": 4,
                    "key_points": ["OOP concepts", "Design patterns", "Real examples"]
                },
                {
                    "question": "Describe a challenging bug you encountered and how you solved it.",
                    "category": "behavioral",
                    "duration": 5,
                    "key_points": ["Problem description", "Debugging process", "Solution approach"]
                },
                {
                    "question": "How do you handle code reviews and feedback from team members?",
                    "category": "behavioral",
                    "duration": 3,
                    "key_points": ["Openness to feedback", "Collaboration skills", "Learning mindset"]
                },
                {
                    "question": "What's your experience with version control systems like Git?",
                    "category": "technical",
                    "duration": 3,
                    "key_points": ["Git workflows", "Branching strategies", "Conflict resolution"]
                },
                {
                    "question": "How do you stay updated with the latest technologies and best practices?",
                    "category": "behavioral",
                    "duration": 3,
                    "key_points": ["Learning methods", "Industry awareness", "Continuous improvement"]
                }
            ],
            'Data Scientist': [
                {
                    "question": "Explain the difference between supervised and unsupervised learning.",
                    "category": "technical",
                    "duration": 4,
                    "key_points": ["Definitions", "Examples", "Use cases"]
                },
                {
                    "question": "Describe a data analysis project you worked on from start to finish.",
                    "category": "behavioral",
                    "duration": 5,
                    "key_points": ["Problem definition", "Data collection", "Analysis process", "Results"]
                },
                {
                    "question": "How do you handle missing or inconsistent data in your analysis?",
                    "category": "technical",
                    "duration": 4,
                    "key_points": ["Data cleaning", "Imputation methods", "Validation"]
                },
                {
                    "question": "What's your experience with statistical testing and hypothesis validation?",
                    "category": "technical",
                    "duration": 4,
                    "key_points": ["Test selection", "P-values", "Interpretation"]
                },
                {
                    "question": "How do you communicate complex technical findings to non-technical stakeholders?",
                    "category": "behavioral",
                    "duration": 3,
                    "key_points": ["Simplification", "Visualization", "Storytelling"]
                }
            ],
            'Product Manager': [
                {
                    "question": "Walk me through how you prioritize features in a product roadmap.",
                    "category": "behavioral",
                    "duration": 4,
                    "key_points": ["Prioritization framework", "Stakeholder input", "Data-driven decisions"]
                },
                {
                    "question": "Describe a time when you had to make a difficult product decision with limited data.",
                    "category": "situational",
                    "duration": 5,
                    "key_points": ["Decision process", "Risk assessment", "Outcome"]
                },
                {
                    "question": "How do you gather and analyze user feedback to inform product decisions?",
                    "category": "behavioral",
                    "duration": 4,
                    "key_points": ["Feedback channels", "Analysis methods", "Action items"]
                },
                {
                    "question": "What's your approach to working with engineering teams and managing technical constraints?",
                    "category": "behavioral",
                    "duration": 4,
                    "key_points": ["Communication", "Understanding constraints", "Collaboration"]
                },
                {
                    "question": "How do you measure the success of a product feature?",
                    "category": "technical",
                    "duration": 4,
                    "key_points": ["KPIs", "Metrics", "Analysis"]
                }
            ]
        }
        
        # General fallback questions for any role
        general_questions = [
            {
                "question": "Tell me about yourself and your professional background.",
                "category": "behavioral",
                "duration": 3,
                "key_points": ["Professional background", "Key skills", "Career goals"]
            },
            {
                "question": "Why are you interested in this position and our company?",
                "category": "behavioral", 
                "duration": 3,
                "key_points": ["Company research", "Role alignment", "Career growth"]
            },
            {
                "question": "Describe a challenging project or problem you worked on. How did you approach it?",
                "category": "behavioral",
                "duration": 4,
                "key_points": ["Problem description", "Solution approach", "Results achieved"]
            },
            {
                "question": "What are your greatest strengths and how would they benefit this role?",
                "category": "behavioral",
                "duration": 3,
                "key_points": ["Relevant strengths", "Specific examples", "Value to employer"]
            },
            {
                "question": "Where do you see yourself in 5 years, and how does this role fit into your career plan?",
                "category": "behavioral",
                "duration": 3,
                "key_points": ["Career progression", "Skill development", "Long-term goals"]
            },
            {
                "question": "Describe a time when you had to work with a difficult team member. How did you handle it?",
                "category": "behavioral",
                "duration": 4,
                "key_points": ["Situation description", "Approach taken", "Resolution outcome"]
            },
            {
                "question": "What do you do to stay motivated and productive when working on long-term projects?",
                "category": "behavioral",
                "duration": 3,
                "key_points": ["Motivation strategies", "Productivity methods", "Results"]
            },
            {
                "question": "How do you handle feedback and criticism in your work?",
                "category": "behavioral",
                "duration": 3,
                "key_points": ["Reception of feedback", "Implementation", "Growth"]
            }
        ]
        
        # Get role-specific questions if available
        questions = role_questions.get(role, [])
        
        # Add general questions to fill the count
        questions.extend(general_questions)
        
        # Filter by question type if specified
        if question_type != "mixed":
            questions = [q for q in questions if q["category"] == question_type]
        
        # Adjust difficulty by modifying questions
        if difficulty == "beginner":
            # Use simpler questions and reduce duration
            for q in questions:
                q["duration"] = max(2, q["duration"] - 1)
        elif difficulty == "advanced":
            # Increase duration for more complex answers
            for q in questions:
                q["duration"] = min(6, q["duration"] + 1)
        
        # Return requested number of questions
        return questions[:count]
    
    def _get_fallback_evaluation(self) -> Dict[str, Any]:
        """Return fallback evaluation when API fails"""
        return {
            "overall_score": 70,
            "scores": {
                "content_quality": 70,
                "communication": 70,
                "depth": 70,
                "relevance": 70
            },
            "strengths": ["Clear communication", "Relevant examples"],
            "improvements": ["Add more specific details", "Structure your answer better"],
            "suggestions": ["Practice the STAR method", "Prepare more concrete examples"]
        }
    
    def _get_fallback_feedback(self) -> Dict[str, Any]:
        """Return fallback feedback when API fails"""
        return {
            "summary": "You demonstrated good communication skills and provided relevant examples.",
            "strengths": ["Clear speaking", "Professional demeanor"],
            "improvements": ["Add more specific details", "Practice storytelling"],
            "action_items": ["Practice common questions", "Prepare STAR format examples"],
            "motivation": "Keep practicing and you'll continue to improve!"
        }    

    def _analyze_performance_and_adjust_difficulty(
        self,
        user_answer: str,
        current_difficulty: str,
        performance_history: List[Dict[str, Any]] = None
    ) -> str:
        """Analyze user performance and adjust difficulty for next questions with enhanced logic"""
        
        if not performance_history:
            return current_difficulty
        
        try:
            # Enhanced performance analysis with multiple factors
            recent_scores = []
            answer_quality_indicators = []
            
            # Analyze recent performance history (last 3-5 answers)
            recent_history = performance_history[-5:] if len(performance_history) > 5 else performance_history
            
            for performance in recent_history:
                overall_score = performance.get('overall_score', 70)
                recent_scores.append(overall_score)
                
                # Analyze specific scoring dimensions
                scores = performance.get('scores', {})
                content_quality = scores.get('content_quality', 70)
                depth = scores.get('depth', 70)
                technical_accuracy = scores.get('technical_accuracy', 70)
                
                # Calculate weighted quality indicator
                quality_score = (content_quality * 0.4 + depth * 0.3 + technical_accuracy * 0.3)
                answer_quality_indicators.append(quality_score)
            
            # Analyze current answer for immediate difficulty adjustment
            current_answer_quality = self._analyze_answer_quality(user_answer)
            
            if recent_scores:
                avg_score = sum(recent_scores) / len(recent_scores)
                avg_quality = sum(answer_quality_indicators) / len(answer_quality_indicators)
                
                # Enhanced difficulty progression logic
                if avg_score >= 85 and avg_quality >= 80 and current_answer_quality >= 75:
                    # Strong performance across all metrics - increase difficulty
                    if current_difficulty == "beginner":
                        return "intermediate"
                    elif current_difficulty == "intermediate":
                        return "advanced"
                    else:
                        return "advanced"  # Stay at advanced
                        
                elif avg_score <= 55 or avg_quality <= 50 or current_answer_quality <= 40:
                    # Poor performance - decrease difficulty
                    if current_difficulty == "advanced":
                        return "intermediate"
                    elif current_difficulty == "intermediate":
                        return "beginner"
                    else:
                        return "beginner"  # Stay at beginner
                        
                elif 60 <= avg_score <= 75 and current_difficulty == "advanced":
                    # Moderate performance at advanced level - step down
                    return "intermediate"
                    
                elif 75 <= avg_score <= 84 and current_difficulty == "beginner":
                    # Good performance at beginner level - step up
                    return "intermediate"
            
            # Consider answer length and technical depth for fine-tuning
            if len(user_answer.split()) < 20 and current_difficulty != "beginner":
                # Very short answers might indicate difficulty
                return "beginner" if current_difficulty == "advanced" else "beginner"
            
            return current_difficulty
            
        except Exception as e:
            logger.error(f"Error analyzing performance: {e}")
            return current_difficulty
    
    def _analyze_answer_quality(self, answer: str) -> float:
        """Analyze the quality of a user's answer to inform difficulty adjustment"""
        
        if not answer or len(answer.strip()) < 10:
            return 20.0  # Very poor quality
        
        try:
            quality_score = 50.0  # Base score
            
            # Length analysis (reasonable detail expected)
            word_count = len(answer.split())
            if word_count >= 50:
                quality_score += 15
            elif word_count >= 25:
                quality_score += 10
            elif word_count >= 10:
                quality_score += 5
            
            # Technical terminology usage
            technical_terms = [
                'algorithm', 'database', 'optimization', 'performance', 'architecture',
                'framework', 'api', 'microservices', 'scalability', 'security',
                'testing', 'deployment', 'monitoring', 'integration', 'design pattern'
            ]
            
            answer_lower = answer.lower()
            technical_term_count = sum(1 for term in technical_terms if term in answer_lower)
            quality_score += min(technical_term_count * 3, 15)  # Max 15 points for technical terms
            
            # Structure indicators (examples, specific details)
            structure_indicators = [
                'for example', 'specifically', 'in my experience', 'i implemented',
                'the approach', 'the solution', 'the challenge', 'the result'
            ]
            
            structure_count = sum(1 for indicator in structure_indicators if indicator in answer_lower)
            quality_score += min(structure_count * 2, 10)  # Max 10 points for structure
            
            # Complexity indicators
            complexity_indicators = [
                'because', 'however', 'therefore', 'additionally', 'furthermore',
                'on the other hand', 'as a result', 'consequently'
            ]
            
            complexity_count = sum(1 for indicator in complexity_indicators if indicator in answer_lower)
            quality_score += min(complexity_count * 2, 10)  # Max 10 points for complexity
            
            return min(quality_score, 100.0)  # Cap at 100
            
        except Exception as e:
            logger.error(f"Error analyzing answer quality: {e}")
            return 50.0  # Default moderate quality
    
    def generate_comprehensive_technical_interview(
        self,
        role_data: Dict[str, Any],
        difficulty: str = "intermediate",
        total_questions: int = 10,
        session_focus: str = "balanced"
    ) -> Dict[str, List[Dict[str, Any]]]:
        """Generate a comprehensive technical interview with different question types"""
        
        try:
            main_role = self._get_role_attribute(role_data, 'main_role', '')
            logger.info(f"Generating comprehensive technical interview for {main_role}")
            
            # Define question distribution based on session focus
            distributions = {
                "balanced": {
                    "database": 0.25,
                    "algorithm": 0.35,
                    "system_design": 0.25,
                    "debugging": 0.15
                },
                "backend_heavy": {
                    "database": 0.4,
                    "algorithm": 0.25,
                    "system_design": 0.25,
                    "debugging": 0.1
                },
                "algorithm_heavy": {
                    "database": 0.15,
                    "algorithm": 0.5,
                    "system_design": 0.2,
                    "debugging": 0.15
                },
                "architecture_heavy": {
                    "database": 0.2,
                    "algorithm": 0.2,
                    "system_design": 0.45,
                    "debugging": 0.15
                }
            }
            
            distribution = distributions.get(session_focus, distributions["balanced"])
            
            # Calculate question counts
            db_count = max(1, int(total_questions * distribution["database"]))
            algo_count = max(1, int(total_questions * distribution["algorithm"]))
            system_count = max(1, int(total_questions * distribution["system_design"]))
            debug_count = max(1, total_questions - db_count - algo_count - system_count)
            
            interview_questions = {
                "database_questions": [],
                "algorithm_questions": [],
                "system_design_questions": [],
                "debugging_questions": [],
                "summary": {
                    "total_questions": total_questions,
                    "difficulty": difficulty,
                    "role": main_role,
                    "distribution": {
                        "database": db_count,
                        "algorithm": algo_count,
                        "system_design": system_count,
                        "debugging": debug_count
                    }
                }
            }
            
            # Generate each type of question
            try:
                interview_questions["database_questions"] = self.generate_database_questions(
                    role_data, difficulty, db_count
                )
                logger.info(f"Generated {len(interview_questions['database_questions'])} database questions")
            except Exception as e:
                logger.error(f"Failed to generate database questions: {e}")
                interview_questions["database_questions"] = []
            
            try:
                interview_questions["algorithm_questions"] = self.generate_algorithm_questions(
                    role_data, difficulty, algo_count
                )
                logger.info(f"Generated {len(interview_questions['algorithm_questions'])} algorithm questions")
            except Exception as e:
                logger.error(f"Failed to generate algorithm questions: {e}")
                interview_questions["algorithm_questions"] = []
            
            try:
                interview_questions["system_design_questions"] = self.generate_system_design_questions(
                    role_data, difficulty, system_count
                )
                logger.info(f"Generated {len(interview_questions['system_design_questions'])} system design questions")
            except Exception as e:
                logger.error(f"Failed to generate system design questions: {e}")
                interview_questions["system_design_questions"] = []
            
            try:
                interview_questions["debugging_questions"] = self.generate_debugging_questions(
                    role_data, difficulty, debug_count
                )
                logger.info(f"Generated {len(interview_questions['debugging_questions'])} debugging questions")
            except Exception as e:
                logger.error(f"Failed to generate debugging questions: {e}")
                interview_questions["debugging_questions"] = []
            
            # Calculate actual totals
            actual_total = (
                len(interview_questions["database_questions"]) +
                len(interview_questions["algorithm_questions"]) +
                len(interview_questions["system_design_questions"]) +
                len(interview_questions["debugging_questions"])
            )
            
            interview_questions["summary"]["actual_total"] = actual_total
            
            logger.info(f"Generated comprehensive technical interview with {actual_total} questions")
            return interview_questions
            
        except Exception as e:
            logger.error(f"Error generating comprehensive technical interview: {str(e)}")
            # Return fallback structure
            return {
                "database_questions": self._get_fallback_database_questions(difficulty, 2, "mixed"),
                "algorithm_questions": self._get_fallback_algorithm_questions(difficulty, 3, "mixed"),
                "system_design_questions": self._get_fallback_system_design_questions(difficulty, 2, "web_application"),
                "debugging_questions": self._get_fallback_debugging_questions(difficulty, 2, "javascript"),
                "summary": {
                    "total_questions": total_questions,
                    "actual_total": 9,
                    "difficulty": difficulty,
                    "role": self._get_role_attribute(role_data, 'main_role', 'Unknown'),
                    "distribution": {"database": 2, "algorithm": 3, "system_design": 2, "debugging": 2},
                    "fallback_used": True
                }
            }
    
    def _get_fallback_questions(
        self, 
        role: str, 
        difficulty: str, 
        question_type: str, 
        count: int
    ) -> List[Dict[str, Any]]:
        """
        Generate intelligent fallback questions based on role and difficulty.
        This method provides meaningful, role-specific questions when AI generation fails.
        """
        logger.info(f"Generating fallback questions for {role}, difficulty: {difficulty}, type: {question_type}")
        
        # Use the dedicated fallback question service
        return self.fallback_service.get_fallback_questions(
            role=role,
            difficulty=difficulty,
            question_type=question_type,
            count=count
        )
    
    def _get_role_specific_fallback_questions(
        self, 
        role: str, 
        difficulty: str
    ) -> List[Dict[str, Any]]:
        """Get role-specific fallback questions organized by role and difficulty"""
        
        # Normalize role name for lookup
        role_key = role.lower().replace(" ", "_").replace("-", "_")
        
        # Role-specific question banks with intelligent selection
        role_questions = {
            "software_developer": {
                "beginner": [
                    {
                        "question": "Tell me about a programming project you've worked on recently. What technologies did you use and what challenges did you face?",
                        "category": "behavioral",
                        "duration": 4,
                        "key_points": ["project description", "technology choices", "problem-solving approach", "learning outcomes"],
                        "role_relevance": "Assesses hands-on programming experience and technical decision-making"
                    },
                    {
                        "question": "How do you approach debugging when your code isn't working as expected?",
                        "category": "technical",
                        "duration": 3,
                        "key_points": ["systematic approach", "debugging tools", "problem isolation", "testing methods"],
                        "role_relevance": "Critical skill for daily development work"
                    },
                    {
                        "question": "Explain the difference between a list and a dictionary in Python, and when you would use each.",
                        "category": "technical",
                        "duration": 3,
                        "key_points": ["data structure characteristics", "use cases", "performance considerations", "practical examples"],
                        "role_relevance": "Fundamental programming concepts essential for development"
                    },
                    {
                        "question": "Describe a time when you had to learn a new programming language or framework. How did you approach it?",
                        "category": "behavioral",
                        "duration": 4,
                        "key_points": ["learning strategy", "resources used", "challenges overcome", "application of knowledge"],
                        "role_relevance": "Demonstrates adaptability and continuous learning mindset"
                    }
                ],
                "intermediate": [
                    {
                        "question": "Walk me through how you would design a REST API for a simple e-commerce application. What endpoints would you create?",
                        "category": "technical",
                        "duration": 5,
                        "key_points": ["API design principles", "HTTP methods", "resource modeling", "error handling", "authentication"],
                        "role_relevance": "Core backend development skill for web applications"
                    },
                    {
                        "question": "Describe a situation where you had to optimize the performance of an application. What was your approach and what were the results?",
                        "category": "behavioral",
                        "duration": 5,
                        "key_points": ["performance analysis", "optimization strategies", "measurement tools", "impact assessment"],
                        "role_relevance": "Performance optimization is crucial for scalable applications"
                    },
                    {
                        "question": "How would you implement user authentication and authorization in a web application? What security considerations would you keep in mind?",
                        "category": "technical",
                        "duration": 5,
                        "key_points": ["authentication methods", "authorization patterns", "security best practices", "token management"],
                        "role_relevance": "Security is fundamental in modern web development"
                    },
                    {
                        "question": "Tell me about a time when you had to work with a difficult codebase or legacy code. How did you handle it?",
                        "category": "behavioral",
                        "duration": 4,
                        "key_points": ["code analysis", "refactoring approach", "risk management", "team collaboration"],
                        "role_relevance": "Common challenge in professional development environments"
                    }
                ],
                "advanced": [
                    {
                        "question": "Design a system that can handle 1 million concurrent users. Walk me through your architecture decisions and trade-offs.",
                        "category": "technical",
                        "duration": 8,
                        "key_points": ["scalability patterns", "load balancing", "database design", "caching strategies", "monitoring"],
                        "role_relevance": "Senior developers must understand large-scale system design"
                    },
                    {
                        "question": "Describe a complex technical decision you made that had significant impact on your team or product. How did you evaluate the options?",
                        "category": "behavioral",
                        "duration": 6,
                        "key_points": ["decision-making process", "stakeholder management", "technical evaluation", "impact measurement"],
                        "role_relevance": "Leadership and technical decision-making for senior roles"
                    },
                    {
                        "question": "How would you approach migrating a monolithic application to microservices? What challenges would you anticipate?",
                        "category": "technical",
                        "duration": 7,
                        "key_points": ["migration strategy", "service boundaries", "data consistency", "deployment patterns", "monitoring"],
                        "role_relevance": "Advanced architectural knowledge for system evolution"
                    }
                ]
            },
            "data_scientist": {
                "beginner": [
                    {
                        "question": "Explain the difference between supervised and unsupervised learning. Can you give examples of when you'd use each?",
                        "category": "technical",
                        "duration": 4,
                        "key_points": ["learning paradigms", "use cases", "algorithm examples", "data requirements"],
                        "role_relevance": "Fundamental machine learning concepts"
                    },
                    {
                        "question": "Walk me through how you would approach a data analysis project from start to finish.",
                        "category": "behavioral",
                        "duration": 5,
                        "key_points": ["project methodology", "data exploration", "analysis techniques", "communication of results"],
                        "role_relevance": "Core data science workflow and project management"
                    },
                    {
                        "question": "How do you handle missing data in a dataset? What are the different approaches and when would you use each?",
                        "category": "technical",
                        "duration": 4,
                        "key_points": ["missing data types", "imputation methods", "impact assessment", "best practices"],
                        "role_relevance": "Common data preprocessing challenge"
                    }
                ],
                "intermediate": [
                    {
                        "question": "Describe a machine learning project you've worked on. What was the business problem, your approach, and the outcome?",
                        "category": "behavioral",
                        "duration": 6,
                        "key_points": ["problem definition", "methodology", "model selection", "evaluation metrics", "business impact"],
                        "role_relevance": "Demonstrates practical ML application and business understanding"
                    },
                    {
                        "question": "How would you evaluate the performance of a classification model? What metrics would you use and why?",
                        "category": "technical",
                        "duration": 5,
                        "key_points": ["evaluation metrics", "cross-validation", "bias-variance tradeoff", "business context"],
                        "role_relevance": "Critical for model validation and deployment decisions"
                    },
                    {
                        "question": "Explain the concept of feature engineering. How do you decide which features to create or select?",
                        "category": "technical",
                        "duration": 5,
                        "key_points": ["feature creation techniques", "selection methods", "domain knowledge", "validation approaches"],
                        "role_relevance": "Key skill for improving model performance"
                    }
                ],
                "advanced": [
                    {
                        "question": "Design an end-to-end machine learning pipeline for a real-time recommendation system. What components would you include?",
                        "category": "technical",
                        "duration": 8,
                        "key_points": ["system architecture", "data pipeline", "model serving", "monitoring", "A/B testing"],
                        "role_relevance": "Senior-level system design and MLOps knowledge"
                    },
                    {
                        "question": "How would you approach building a machine learning model when you have very limited labeled data?",
                        "category": "technical",
                        "duration": 6,
                        "key_points": ["few-shot learning", "transfer learning", "data augmentation", "active learning", "semi-supervised methods"],
                        "role_relevance": "Advanced ML techniques for data-scarce scenarios"
                    }
                ]
            },
            "devops_engineer": {
                "beginner": [
                    {
                        "question": "Explain what CI/CD means and why it's important in software development.",
                        "category": "technical",
                        "duration": 4,
                        "key_points": ["continuous integration", "continuous deployment", "automation benefits", "pipeline components"],
                        "role_relevance": "Fundamental DevOps concept and practice"
                    },
                    {
                        "question": "Describe your experience with version control systems. How do you handle branching and merging?",
                        "category": "behavioral",
                        "duration": 4,
                        "key_points": ["Git workflows", "branching strategies", "merge conflict resolution", "collaboration practices"],
                        "role_relevance": "Essential tool for code management and team collaboration"
                    },
                    {
                        "question": "What is containerization and how does Docker help in application deployment?",
                        "category": "technical",
                        "duration": 4,
                        "key_points": ["containerization concepts", "Docker benefits", "image management", "deployment advantages"],
                        "role_relevance": "Core technology for modern application deployment"
                    }
                ],
                "intermediate": [
                    {
                        "question": "Walk me through how you would set up monitoring and alerting for a web application in production.",
                        "category": "technical",
                        "duration": 6,
                        "key_points": ["monitoring tools", "key metrics", "alerting strategies", "incident response"],
                        "role_relevance": "Critical for maintaining production system reliability"
                    },
                    {
                        "question": "Describe a time when you had to troubleshoot a production issue. What was your approach?",
                        "category": "behavioral",
                        "duration": 5,
                        "key_points": ["incident response", "debugging methodology", "communication", "post-mortem analysis"],
                        "role_relevance": "Essential skill for production support and reliability"
                    },
                    {
                        "question": "How would you implement Infrastructure as Code? What tools would you use and why?",
                        "category": "technical",
                        "duration": 5,
                        "key_points": ["IaC principles", "tool selection", "version control", "testing strategies"],
                        "role_relevance": "Modern approach to infrastructure management"
                    }
                ],
                "advanced": [
                    {
                        "question": "Design a highly available, scalable infrastructure for a global e-commerce platform. What would be your key considerations?",
                        "category": "technical",
                        "duration": 8,
                        "key_points": ["high availability patterns", "global distribution", "auto-scaling", "disaster recovery", "cost optimization"],
                        "role_relevance": "Senior-level infrastructure design and architecture"
                    },
                    {
                        "question": "How would you implement a zero-downtime deployment strategy for a critical production system?",
                        "category": "technical",
                        "duration": 6,
                        "key_points": ["deployment strategies", "blue-green deployment", "canary releases", "rollback procedures", "health checks"],
                        "role_relevance": "Advanced deployment practices for mission-critical systems"
                    }
                ]
            },
            "product_manager": {
                "beginner": [
                    {
                        "question": "How do you prioritize features when you have limited development resources?",
                        "category": "behavioral",
                        "duration": 4,
                        "key_points": ["prioritization frameworks", "stakeholder management", "resource allocation", "impact assessment"],
                        "role_relevance": "Core PM responsibility for product roadmap management"
                    },
                    {
                        "question": "Describe how you would gather and analyze user requirements for a new feature.",
                        "category": "behavioral",
                        "duration": 5,
                        "key_points": ["user research methods", "requirement gathering", "analysis techniques", "validation approaches"],
                        "role_relevance": "Essential skill for understanding user needs and defining solutions"
                    },
                    {
                        "question": "What metrics would you use to measure the success of a mobile app feature?",
                        "category": "technical",
                        "duration": 4,
                        "key_points": ["success metrics", "KPI definition", "measurement tools", "data analysis"],
                        "role_relevance": "Data-driven decision making and feature evaluation"
                    }
                ],
                "intermediate": [
                    {
                        "question": "Tell me about a time when you had to make a difficult product decision with incomplete information. How did you approach it?",
                        "category": "behavioral",
                        "duration": 5,
                        "key_points": ["decision-making process", "risk assessment", "stakeholder communication", "outcome evaluation"],
                        "role_relevance": "Common PM challenge requiring judgment and leadership"
                    },
                    {
                        "question": "How would you work with engineering teams to estimate and plan a complex feature development?",
                        "category": "behavioral",
                        "duration": 5,
                        "key_points": ["technical collaboration", "estimation techniques", "project planning", "risk management"],
                        "role_relevance": "Cross-functional collaboration and project management"
                    },
                    {
                        "question": "Describe your approach to conducting user research and how you translate findings into product decisions.",
                        "category": "behavioral",
                        "duration": 5,
                        "key_points": ["research methodology", "data interpretation", "insight generation", "decision implementation"],
                        "role_relevance": "User-centered product development and research skills"
                    }
                ],
                "advanced": [
                    {
                        "question": "How would you develop a go-to-market strategy for a new product in a competitive market?",
                        "category": "behavioral",
                        "duration": 7,
                        "key_points": ["market analysis", "competitive positioning", "launch strategy", "success metrics", "risk mitigation"],
                        "role_relevance": "Strategic product leadership and market understanding"
                    },
                    {
                        "question": "Describe how you would build and manage a product roadmap for a platform serving multiple customer segments.",
                        "category": "behavioral",
                        "duration": 6,
                        "key_points": ["roadmap strategy", "stakeholder alignment", "resource allocation", "trade-off decisions"],
                        "role_relevance": "Senior PM responsibility for strategic planning and execution"
                    }
                ]
            }
        }
        
        # Get questions for the specific role and difficulty
        role_questions_data = role_questions.get(role_key, {})
        difficulty_questions = role_questions_data.get(difficulty, [])
        
        # If no specific questions found, try to get from a similar role or difficulty
        if not difficulty_questions:
            # Try other difficulty levels for the same role
            for diff in ["beginner", "intermediate", "advanced"]:
                if diff != difficulty and role_questions_data.get(diff):
                    difficulty_questions = role_questions_data[diff][:2]  # Take fewer from different difficulty
                    break
        
        return difficulty_questions
    
    def _get_generic_fallback_questions(
        self, 
        difficulty: str, 
        question_type: str
    ) -> List[Dict[str, Any]]:
        """Get generic fallback questions when role-specific ones are insufficient"""
        
        generic_questions = {
            "behavioral": {
                "beginner": [
                    {
                        "question": "Tell me about yourself and why you're interested in this role.",
                        "category": "behavioral",
                        "duration": 3,
                        "key_points": ["background summary", "career motivation", "role alignment", "communication skills"],
                        "role_relevance": "Universal opener to assess communication and self-awareness"
                    },
                    {
                        "question": "Describe a challenge you faced in a previous role and how you overcame it.",
                        "category": "behavioral",
                        "duration": 4,
                        "key_points": ["problem identification", "solution approach", "outcome", "learning"],
                        "role_relevance": "Assesses problem-solving and resilience"
                    }
                ],
                "intermediate": [
                    {
                        "question": "Tell me about a time when you had to work with a difficult team member. How did you handle the situation?",
                        "category": "behavioral",
                        "duration": 4,
                        "key_points": ["conflict resolution", "communication skills", "teamwork", "professional maturity"],
                        "role_relevance": "Evaluates interpersonal skills and team collaboration"
                    },
                    {
                        "question": "Describe a project where you had to learn something new quickly. What was your approach?",
                        "category": "behavioral",
                        "duration": 4,
                        "key_points": ["learning agility", "adaptation", "resource utilization", "time management"],
                        "role_relevance": "Assesses adaptability and continuous learning mindset"
                    }
                ],
                "advanced": [
                    {
                        "question": "Tell me about a time when you had to make a decision that was unpopular with your team. How did you handle it?",
                        "category": "behavioral",
                        "duration": 5,
                        "key_points": ["leadership", "decision-making", "stakeholder management", "communication"],
                        "role_relevance": "Evaluates leadership and decision-making under pressure"
                    },
                    {
                        "question": "Describe a situation where you had to influence others without having direct authority over them.",
                        "category": "behavioral",
                        "duration": 5,
                        "key_points": ["influence skills", "persuasion", "relationship building", "strategic thinking"],
                        "role_relevance": "Assesses leadership and influence capabilities"
                    }
                ]
            },
            "technical": {
                "beginner": [
                    {
                        "question": "What programming languages or technologies are you most comfortable with, and why?",
                        "category": "technical",
                        "duration": 3,
                        "key_points": ["technical skills", "learning preferences", "practical experience", "technology choices"],
                        "role_relevance": "Baseline technical competency assessment"
                    }
                ],
                "intermediate": [
                    {
                        "question": "How do you stay current with new technologies and industry trends?",
                        "category": "technical",
                        "duration": 3,
                        "key_points": ["continuous learning", "industry awareness", "skill development", "professional growth"],
                        "role_relevance": "Assesses commitment to professional development"
                    }
                ],
                "advanced": [
                    {
                        "question": "Describe your approach to technical decision-making when there are multiple viable solutions.",
                        "category": "technical",
                        "duration": 5,
                        "key_points": ["evaluation criteria", "trade-off analysis", "stakeholder consideration", "long-term thinking"],
                        "role_relevance": "Senior-level technical judgment and decision-making"
                    }
                ]
            },
            "situational": [
                {
                    "question": "How would you handle a situation where you're given a tight deadline but insufficient resources?",
                    "category": "situational",
                    "duration": 4,
                    "key_points": ["resource management", "prioritization", "stakeholder communication", "creative solutions"],
                    "role_relevance": "Common workplace challenge requiring strategic thinking"
                },
                {
                    "question": "What would you do if you discovered a significant error in work that had already been delivered to a client?",
                    "category": "situational",
                    "duration": 4,
                    "key_points": ["accountability", "problem-solving", "client communication", "damage control"],
                    "role_relevance": "Tests integrity and crisis management skills"
                }
            ]
        }
        
        questions = []
        
        if question_type == "mixed" or question_type == "behavioral":
            questions.extend(generic_questions["behavioral"].get(difficulty, []))
        
        if question_type == "mixed" or question_type == "technical":
            questions.extend(generic_questions["technical"].get(difficulty, []))
        
        if question_type == "mixed" or question_type == "situational":
            questions.extend(generic_questions["situational"])
        
        return questions
    
    def _validate_fallback_question_quality(
        self, 
        questions: List[Dict[str, Any]], 
        role: str, 
        difficulty: str
    ) -> List[Dict[str, Any]]:
        """
        Validate and ensure quality of fallback questions.
        Filters out low-quality questions and ensures role relevance.
        """
        validated_questions = []
        
        for question in questions:
            # Quality validation criteria
            if self._is_high_quality_fallback_question(question, role, difficulty):
                validated_questions.append(question)
        
        # Ensure we have diverse question types
        validated_questions = self._ensure_question_diversity(validated_questions)
        
        # Sort by relevance score (if available) or use default ordering
        validated_questions = self._sort_by_relevance(validated_questions, role, difficulty)
        
        return validated_questions
    
    def _is_high_quality_fallback_question(
        self, 
        question: Dict[str, Any], 
        role: str, 
        difficulty: str
    ) -> bool:
        """Check if a fallback question meets quality standards"""
        
        # Basic structure validation
        required_fields = ["question", "category", "duration", "key_points"]
        if not all(field in question for field in required_fields):
            return False
        
        # Content quality checks
        question_text = question.get("question", "")
        
        # Minimum length check
        if len(question_text) < 20:
            return False
        
        # Avoid overly generic questions
        generic_phrases = [
            "tell me about yourself",
            "what are your strengths",
            "what are your weaknesses",
            "where do you see yourself"
        ]
        
        question_lower = question_text.lower()
        generic_count = sum(1 for phrase in generic_phrases if phrase in question_lower)
        
        # Allow some generic questions but prefer specific ones
        if generic_count > 0 and len(question.get("key_points", [])) < 3:
            return False
        
        # Duration validation
        duration = question.get("duration", 0)
        if duration < 2 or duration > 10:
            return False
        
        # Key points validation
        key_points = question.get("key_points", [])
        if len(key_points) < 2:
            return False
        
        return True
    
    def _ensure_question_diversity(
        self, 
        questions: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Ensure diversity in question types and categories"""
        
        if len(questions) <= 3:
            return questions
        
        # Group by category
        by_category = {}
        for question in questions:
            category = question.get("category", "unknown")
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(question)
        
        # Ensure balanced representation
        diverse_questions = []
        max_per_category = max(2, len(questions) // len(by_category))
        
        for category, category_questions in by_category.items():
            diverse_questions.extend(category_questions[:max_per_category])
        
        return diverse_questions
    
    def _sort_by_relevance(
        self, 
        questions: List[Dict[str, Any]], 
        role: str, 
        difficulty: str
    ) -> List[Dict[str, Any]]:
        """Sort questions by relevance to role and difficulty"""
        
        def relevance_score(question):
            score = 0
            
            # Role relevance
            role_relevance = question.get("role_relevance", "")
            if role.lower() in role_relevance.lower():
                score += 10
            
            # Difficulty appropriateness
            duration = question.get("duration", 3)
            if difficulty == "beginner" and duration <= 4:
                score += 5
            elif difficulty == "intermediate" and 3 <= duration <= 6:
                score += 5
            elif difficulty == "advanced" and duration >= 5:
                score += 5
            
            # Key points quality
            key_points = question.get("key_points", [])
            score += min(len(key_points), 5)
            
            # Technical questions get higher score for technical roles
            technical_roles = ["software_developer", "data_scientist", "devops_engineer"]
            if any(tech_role in role.lower() for tech_role in technical_roles):
                if question.get("category") == "technical":
                    score += 3
            
            return score
        
        return sorted(questions, key=relevance_score, reverse=True)
    
    def _get_fallback_evaluation(self) -> Dict[str, Any]:
        """Provide fallback evaluation when AI evaluation fails"""
        return {
            "overall_score": 70,
            "content_quality": 70,
            "communication": 70,
            "technical_accuracy": 70,
            "feedback": "Your response has been recorded. Due to technical limitations, detailed evaluation is not available at this time.",
            "suggestions": [
                "Continue practicing to improve your interview skills",
                "Focus on providing specific examples in your answers",
                "Practice articulating your thoughts clearly and concisely"
            ],
            "fallback_used": True
        }
    
    def _get_fallback_feedback(self) -> Dict[str, Any]:
        """Provide fallback feedback when AI feedback generation fails"""
        return {
            "overall_performance": "Your interview responses have been recorded successfully.",
            "strengths": [
                "Participated in the interview process",
                "Provided responses to all questions"
            ],
            "areas_for_improvement": [
                "Continue practicing interview skills",
                "Focus on providing specific examples",
                "Work on clear communication"
            ],
            "recommendations": [
                "Practice common interview questions",
                "Prepare specific examples from your experience",
                "Record yourself to improve delivery"
            ],
            "fallback_used": True
        }
    
    def _get_fallback_comprehensive_feedback(
        self, 
        performance_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate comprehensive fallback feedback based on available performance data"""
        
        # Extract basic metrics if available
        sessions_completed = performance_data.get("sessions_completed", 1)
        avg_score = performance_data.get("average_score", 70)
        
        return {
            "overall_assessment": f"You have completed {sessions_completed} interview session(s) with an average performance score of {avg_score}%.",
            "performance_trends": {
                "improvement_areas": [
                    "Continue regular practice to build confidence",
                    "Focus on providing structured responses",
                    "Develop a portfolio of specific examples"
                ],
                "strengths": [
                    "Consistent participation in practice sessions",
                    "Engagement with the interview process"
                ]
            },
            "detailed_feedback": {
                "content_quality": "Focus on providing specific, relevant examples that demonstrate your skills and experience.",
                "communication": "Practice clear and concise communication. Structure your responses with a clear beginning, middle, and end.",
                "technical_skills": "Continue developing your technical knowledge and stay current with industry trends."
            },
            "action_plan": [
                "Schedule regular practice sessions",
                "Prepare and practice specific examples from your experience",
                "Research common interview questions for your target role",
                "Consider mock interviews with peers or mentors"
            ],
            "resources": [
                "Practice behavioral interview questions using the STAR method",
                "Review technical concepts relevant to your field",
                "Join professional communities and networking groups"
            ],
            "fallback_used": True,
            "generated_at": datetime.now().isoformat()
        }
    
    def build_technical_question_prompt(self, context: Dict[str, Any]) -> str:
        """Build comprehensive technical question generation prompt with rich user context"""
        try:
            user_profile = context['user_profile']
            session_context = context['session_context']
            performance_history = context['performance_history']
            question_requirements = context['question_requirements']
            
            role_hierarchy = user_profile['role_hierarchy']
            tech_stacks = user_profile.get('tech_stacks', [])
            
            prompt = f"""
You are an expert technical interviewer conducting a specialized interview with comprehensive candidate context.

CANDIDATE ROLE HIERARCHY:
- Main Role: {role_hierarchy['main_role']}
- Sub Role: {role_hierarchy['sub_role']}
- Specialization: {role_hierarchy['specialization']}
- Required Technologies: {', '.join(tech_stacks[:7]) if tech_stacks else 'Not specified'}
- Experience Level: {user_profile['experience_level']}

SESSION CONTEXT:
- Session Type: {session_context['session_type']}
- Target Question Count: {session_context['question_count']}
- Current Difficulty Level: {session_context['current_difficulty']}
- Time Limit: {session_context['time_limit']} minutes

CRITICAL REQUIREMENTS:
1. Generate EXACTLY {session_context['question_count']} questions
2. Each question MUST be directly relevant to the candidate's role hierarchy and competencies
3. Questions MUST match the {session_context['current_difficulty']} difficulty level precisely
4. NO generic questions that could apply to any role - use the rich context provided
5. Each question should test different aspects of the role-specific competencies

OUTPUT FORMAT:
Provide your response as a JSON array with this exact structure:
[
    {{
        "question": "Specific, role-hierarchy-targeted question text here",
        "category": "behavioral|technical|situational|problem-solving|system-design",
        "duration": 3,
        "key_points": ["specific competency 1", "technical skill 2", "behavioral indicator 3"],
        "role_relevance": "Why this question is essential for this specific role hierarchy"
    }}
]

Generate {session_context['question_count']} role-optimized questions now:
"""
            return prompt.strip()
            
        except Exception as e:
            logger.error(f"Error building technical question prompt: {str(e)}")
            raise

    def build_contextual_followup_prompt(self, context: Dict[str, Any], previous_qa: Dict[str, Any]) -> str:
        """Build contextual follow-up question prompt based on previous answer"""
        try:
            user_profile = context['user_profile']
            session_context = context['session_context']
            
            previous_question = previous_qa.get('question', '')
            user_answer = previous_qa.get('answer', '')
            
            role_hierarchy = user_profile['role_hierarchy']
            
            prompt = f"""
You are conducting a technical interview and need to generate intelligent follow-up questions based on the candidate's response.

CANDIDATE ROLE CONTEXT:
- Main Role: {role_hierarchy['main_role']}
- Sub Role: {role_hierarchy['sub_role']}
- Specialization: {role_hierarchy['specialization']}
- Experience Level: {user_profile['experience_level']}

PREVIOUS QUESTION:
{previous_question}

CANDIDATE'S ANSWER:
{user_answer}

FOLLOW-UP REQUIREMENTS:
1. Generate 1-2 follow-up questions that naturally build on their response
2. Dive deeper into specific technologies, projects, or concepts they mentioned
3. Test their understanding at a deeper level appropriate for {session_context['current_difficulty']} difficulty
4. Maintain relevance to their role hierarchy and specialization

OUTPUT FORMAT:
[
    {{
        "question": "Natural follow-up question based on their response",
        "category": "behavioral|technical|situational|problem-solving",
        "duration": 2,
        "key_points": ["specific aspect to explore", "deeper understanding check"],
        "followup_rationale": "Why this follow-up is valuable based on their answer"
    }}
]

Generate intelligent follow-up questions now:
"""
            return prompt.strip()
            
        except Exception as e:
            logger.error(f"Error building contextual followup prompt: {str(e)}")
            raise

    def build_validation_prompt(self, questions: List[Dict[str, Any]], context: Dict[str, Any]) -> str:
        """Build prompt for validating generated questions against user context"""
        try:
            user_profile = context['user_profile']
            session_context = context['session_context']
            
            role_hierarchy = user_profile['role_hierarchy']
            
            questions_text = ""
            for i, q in enumerate(questions, 1):
                questions_text += f"""
Question {i}:
- Text: {q.get('question', 'N/A')}
- Category: {q.get('category', 'N/A')}
- Duration: {q.get('duration', 'N/A')} minutes
- Key Points: {', '.join(q.get('key_points', []))}
"""
            
            prompt = f"""
You are a technical interview quality assurance expert. Validate these questions against the candidate's profile and requirements.

CANDIDATE PROFILE:
- Role: {role_hierarchy['main_role']} > {role_hierarchy['sub_role']} > {role_hierarchy['specialization']}
- Experience Level: {user_profile['experience_level']}
- Current Difficulty: {session_context['current_difficulty']}

QUESTIONS TO VALIDATE:
{questions_text}

VALIDATION CRITERIA:
1. Role Relevance: Must be directly applicable to {role_hierarchy['main_role']} work
2. Specialization Alignment: Should test {role_hierarchy['specialization']} specific knowledge
3. Experience Appropriateness: Suitable for {user_profile['experience_level']} level
4. Difficulty Match: Must match {session_context['current_difficulty']} complexity level

OUTPUT FORMAT:
{{
    "validated_questions": [
        // Only questions that pass ALL validation criteria
    ],
    "rejected_questions": [
        {{
            "question": "rejected question text",
            "rejection_reason": "specific reason for rejection"
        }}
    ],
    "validation_summary": {{
        "total_questions": {len(questions)},
        "passed_validation": 0,
        "rejected_count": 0,
        "overall_quality": "excellent|good|needs_improvement|poor"
    }}
}}

Perform validation now:
"""
            return prompt.strip()
            
        except Exception as e:
            logger.error(f"Error building validation prompt: {str(e)}")
            raise

    def build_answer_evaluation_prompt(self, context: Dict[str, Any], qa_data: Dict[str, Any]) -> str:
        """Build prompt for evaluating user answers with rich context"""
        try:
            user_profile = context['user_profile']
            
            question = qa_data.get('question', '')
            answer = qa_data.get('answer', '')
            
            role_hierarchy = user_profile['role_hierarchy']
            
            prompt = f"""
You are an expert technical interviewer evaluating a candidate's answer with comprehensive context.

CANDIDATE CONTEXT:
- Role: {role_hierarchy['main_role']} > {role_hierarchy['sub_role']} > {role_hierarchy['specialization']}
- Experience Level: {user_profile['experience_level']}

QUESTION ASKED:
{question}

CANDIDATE'S ANSWER:
{answer}

EVALUATION REQUIREMENTS:
1. Assess technical accuracy and completeness
2. Evaluate communication clarity and structure
3. Consider the candidate's experience level and role requirements
4. Identify strengths and specific improvement areas
5. Provide actionable feedback aligned with their career goals

OUTPUT FORMAT:
{{
    "scores": {{
        "technical_accuracy": 85,
        "completeness": 75,
        "communication_clarity": 90,
        "role_relevance": 80,
        "overall_score": 82
    }},
    "strengths": [
        "Specific strength observed in the answer"
    ],
    "improvement_areas": [
        "Specific area needing improvement"
    ],
    "feedback": "Detailed, constructive feedback tailored to their role and experience level",
    "competency_assessment": {{
        "demonstrated_competencies": ["competency1", "competency2"],
        "missing_competencies": ["competency3", "competency4"]
    }}
}}

Evaluate the answer now:
"""
            return prompt.strip()
            
        except Exception as e:
            logger.error(f"Error building answer evaluation prompt: {str(e)}")
            raise